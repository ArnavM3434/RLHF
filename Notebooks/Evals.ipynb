{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccrDmrVoME5C",
        "outputId": "e84af389-09d5-423b-b952-ea1edc3b31eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trl==0.11\n",
            "  Downloading trl-0.11.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Collecting tyro>=0.5.11 (from trl==0.11)\n",
            "  Downloading tyro-0.9.35-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.12/dist-packages (from trl==0.11) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.11) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.11) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.11)\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.11) (4.4.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11) (0.1.2)\n",
            "Downloading trl-0.11.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.4/316.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.35-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: shtab, tyro, trl\n",
            "Successfully installed shtab-1.7.2 trl-0.11.0 tyro-0.9.35\n"
          ]
        }
      ],
      "source": [
        "!pip install trl==0.11 transformers accelerate datasets torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any\n",
        "import glob\n",
        "import shutil\n",
        "import wandb\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "W0Es7AiQMMA9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNr4Fw4ANOPC",
        "outputId": "a3bb282d-f2dc-458a-98da-9f2897b1c223"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from rouge) (1.17.0)\n",
            "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5TQrowOMOtN",
        "outputId": "5dc011df-1934-45f3-fdb5-7c76dd127f70"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWtcL89QOPCc",
        "outputId": "b1e05c1c-e671-4c73-c0f6-fe03225bbfd8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "workspace = 'PPO-Real'"
      ],
      "metadata": {
        "id": "4e4Brz01UrVN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(f\"/content/drive/My Drive/{workspace}\")\n",
        "print(\"Current working dir:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp4EGWphUt3L",
        "outputId": "d3ac1f5b-e13d-4f6e-b8c0-b92f3f0ef9ab"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working dir: /content/drive/My Drive/PPO-Real\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "gpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "sft_model = AutoModelForCausalLM.from_pretrained(\"ArnavM3434/gpt2-alpaca-second-try\").to(device)"
      ],
      "metadata": {
        "id": "S2lMzxB9OdNz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Qualitative First"
      ],
      "metadata": {
        "id": "W7EuHFWNQtmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_with_model(model, tokenizer, prompt, max_new_tokens=100):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    if \"Assistant:\" in response:\n",
        "        response = response.split(\"Assistant:\")[1].strip()\n",
        "    return response\n",
        "\n",
        "test_prompts = [\n",
        "    \"Human: Explain quantum physics in simple terms\\nAssistant:\",\n",
        "    \"Human: Write a short poem about AI\\nAssistant:\",\n",
        "    \"Human: How do I make a chocolate cake?\\nAssistant:\",\n",
        "    \"Human: What are the benefits of exercise?\\nAssistant:\",\n",
        "    \"Human: Summarize the plot of Romeo and Juliet\\nAssistant:\"\n",
        "]\n",
        "\n",
        "print(\"=== Testing GPT-2 ===\")\n",
        "for prompt in test_prompts:\n",
        "    response = generate_with_model(gpt2_model, tokenizer, prompt)\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Response: {response}\\n\")\n",
        "\n",
        "print(\"=== Testing SFT Model ===\")\n",
        "for prompt in test_prompts:\n",
        "    response = generate_with_model(sft_model, tokenizer, prompt)\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Response: {response}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgbtJ8X4MSBk",
        "outputId": "6ae2e42d-2b8e-415d-a292-e4db0a52350e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Testing GPT-2 ===\n",
            "Prompt: Human: Explain quantum physics in simple terms\n",
            "Assistant:\n",
            "Response: Don't let your ego get in the way of your scientific and technical abilities – it's just what you need.\n",
            "\n",
            "Prompt: Human: Write a short poem about AI\n",
            "Assistant:\n",
            "Response: Write an essay about AI.\n",
            "\n",
            "Prompt: Human: How do I make a chocolate cake?\n",
            "Assistant:\n",
            "Response: To make a chocolate cake, I use a cream cake batter and a granulated sugar. I then use a mixture of two baking powder and 2 g of water to make chocolate icing. The mixture is then poured over the cake. The icing is then shaped into a cake, and I then place the cake back on top in a warm, moist place.\n",
            "Here is how to make a chocolate cake:\n",
            "1. Preheat oven to 350 F.\n",
            "2. Preheat oven to 350 F\n",
            "\n",
            "Prompt: Human: What are the benefits of exercise?\n",
            "Assistant:\n",
            "Response: I do not have the answers to that. I don't have any of the answers, and I don't believe you should do that. I don't believe that you should do that to yourself. I believe you should be able to choose to participate in a sports program. That's what you get.\n",
            "I have a family background, I have a husband and a daughter that I attended college with. I have a family that I grew up with, I have a wife who is a nurse who\n",
            "\n",
            "Prompt: Human: Summarize the plot of Romeo and Juliet\n",
            "Assistant:\n",
            "Response: The D.C. police chief\n",
            "Assistant Crime Scene Analyst: The D.C. police chief\n",
            "Assistant Culprit: The D.C. police chief\n",
            "Assistant Culprit: The D.C. police chief\n",
            "Assistant Culprit: The D.C. police chief\n",
            "Assistant Culprit: The D.C. police chief\n",
            "Assistant Culprit: The D.C. police chief\n",
            "Assistant Culprit: The D.C. police chief\n",
            "Assistant\n",
            "\n",
            "=== Testing SFT Model ===\n",
            "Prompt: Human: Explain quantum physics in simple terms\n",
            "Assistant:\n",
            "Response: Quantum mechanics is an integral field theory of particle physics, which describes the interaction of particles in a binary world. It is a branch of physics in particle physics that involves particles interacting in a quantum state. Quantum mechanics is based on the theory of the qubit, which is a process where particles are excited by an intense quantum state. It takes place in a vacuum, and is a basic form of measurement and measurement. It is known as \"quantum mechanics.\"\n",
            "\n",
            "Prompt: Human: Write a short poem about AI\n",
            "Assistant:\n",
            "Response: A machine's mind knows what it knows, but it can't do anything.\n",
            "\n",
            "Prompt: Human: How do I make a chocolate cake?\n",
            "Assistant:\n",
            "Response: You can make a chocolate cake by mixing a cup of sugar and vanilla extract into a small bowl. You can use a wooden spoon or a fork to scoop out the mixture, and add the mixture to a large mixing bowl. Once the mixture has a smooth consistency, add a handful of cream and mix until combined. Fold in the chocolate and continue mixing until the mixture is smooth. Once the mixture is smooth, add a dollop of powdered sugar and mix until it has a smooth consistency. Add in\n",
            "\n",
            "Prompt: Human: What are the benefits of exercise?\n",
            "Assistant:\n",
            "Response: The benefits of exercise include improved cardiovascular health, improved mental health, improved concentration, and improved sleep, as well as general well-being.\n",
            "\n",
            "Prompt: Human: Summarize the plot of Romeo and Juliet\n",
            "Assistant:\n",
            "Response: Romeo and Juliet is a story about love, friendship, love, and friendship. It reveals the true meaning of love and how it can be used to advance one's own personal interests and goals.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bleu Score"
      ],
      "metadata": {
        "id": "4i7w65c0QwcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install alpaca_eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3FrYxn7Qy6V",
        "outputId": "8fe1dd41-f363-4714-b438-b5dbda8d6680"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting alpaca_eval\n",
            "  Downloading alpaca_eval-0.6.6-py3-none-any.whl.metadata (87 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/87.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.7/87.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from alpaca_eval) (1.1.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from alpaca_eval) (4.0.0)\n",
            "Requirement already satisfied: openai>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from alpaca_eval) (1.109.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from alpaca_eval) (2.2.2)\n",
            "Requirement already satisfied: tiktoken>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from alpaca_eval) (0.12.0)\n",
            "Collecting fire (from alpaca_eval)\n",
            "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from alpaca_eval) (1.16.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from alpaca_eval) (0.35.3)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.12/dist-packages (from alpaca_eval) (1.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from alpaca_eval) (1.6.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.5.0->alpaca_eval) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.5.0->alpaca_eval) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.5.0->alpaca_eval) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.5.0->alpaca_eval) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.5.0->alpaca_eval) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.5.0->alpaca_eval) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai>=1.5.0->alpaca_eval) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai>=1.5.0->alpaca_eval) (4.15.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.3.2->alpaca_eval) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.3.2->alpaca_eval) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets->alpaca_eval) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets->alpaca_eval) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->alpaca_eval) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->alpaca_eval) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->alpaca_eval) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->alpaca_eval) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->alpaca_eval) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets->alpaca_eval) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets->alpaca_eval) (6.0.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->alpaca_eval) (1.1.10)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from fire->alpaca_eval) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->alpaca_eval) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->alpaca_eval) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->alpaca_eval) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->alpaca_eval) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->alpaca_eval) (3.6.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai>=1.5.0->alpaca_eval) (3.11)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->alpaca_eval) (3.13.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=1.5.0->alpaca_eval) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=1.5.0->alpaca_eval) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.5.0->alpaca_eval) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.5.0->alpaca_eval) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.5.0->alpaca_eval) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.5.0->alpaca_eval) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->alpaca_eval) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken>=0.3.2->alpaca_eval) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken>=0.3.2->alpaca_eval) (2.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->alpaca_eval) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->alpaca_eval) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->alpaca_eval) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->alpaca_eval) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->alpaca_eval) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->alpaca_eval) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->alpaca_eval) (1.22.0)\n",
            "Downloading alpaca_eval-0.6.6-py3-none-any.whl (361 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m361.7/361.7 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fire, alpaca_eval\n",
            "Successfully installed alpaca_eval-0.6.6 fire-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "import numpy as np\n",
        "\n",
        "bleu = load(\"bleu\")\n",
        "\n",
        "def evaluate_with_references(model, tokenizer, test_data):\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    for example in test_data[:10]:\n",
        "        prompt = example[\"instruction\"]  #\n",
        "        reference = example[\"output\"]\n",
        "\n",
        "        full_prompt = prompt\n",
        "\n",
        "        inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=100,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                do_sample=True,\n",
        "                temperature=0.7\n",
        "            )\n",
        "\n",
        "        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        if \"Assistant:\" in full_output:\n",
        "            prediction = full_output.split(\"Assistant:\")[1].strip()\n",
        "        else:\n",
        "            prediction = full_output.replace(prompt, \"\").strip()\n",
        "\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        print(f\"Full output: {full_output}\")\n",
        "        print(f\"Extracted prediction: {prediction}\")\n",
        "        print(f\"Reference: {reference}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        predictions.append(prediction)\n",
        "\n",
        "        if \"Assistant:\" in reference:\n",
        "            ref_response = reference.split(\"Assistant:\")[1].strip()\n",
        "        else:\n",
        "            ref_response = reference\n",
        "        references.append([ref_response])\n",
        "\n",
        "    bleu_score = bleu.compute(predictions=predictions, references=references)\n",
        "\n",
        "    return {\"bleu\": bleu_score}\n",
        "\n",
        "# Test data formatted exactly like your training data\n",
        "test_data = [\n",
        "    {\"instruction\": \"Human: Explain gravity simply Assistant: \", \"output\": \"Assistant: Gravity is the force that pulls objects toward each other. It's what keeps us on the ground.\"},\n",
        "    {\"instruction\": \"Human: Write a haiku about programming Assistant: \", \"output\": \"Assistant: Code flows like water\\nBugs hide in dark corners\\nFix and compile\"},\n",
        "    {\"instruction\": \"Human: What is the capital of France? Assistant: \", \"output\": \"Assistant: The capital of France is Paris.\"},\n",
        "    {\"instruction\": \"Human: How do you make a cup of tea? Assistant: \", \"output\": \"Assistant: Boil water, add a tea bag to a cup, pour hot water over it, steep for 3-5 minutes, then remove the tea bag.\"},\n",
        "]\n",
        "\n",
        "print(\"Evaluating GPT-2...\")\n",
        "gpt2_scores = evaluate_with_references(gpt2_model, tokenizer, test_data)\n",
        "\n",
        "print(\"\\nEvaluating SFT model...\")\n",
        "sft_scores = evaluate_with_references(sft_model, tokenizer, test_data)\n",
        "\n",
        "print(\"\\n=== RESULTS ===\")\n",
        "print(\"GPT-2:\", gpt2_scores)\n",
        "print(\"SFT:\", sft_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQNaxwKiNBHA",
        "outputId": "1eda88e8-6d91-471d-cfad-2654a1a0cca3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating GPT-2...\n",
            "Prompt: Human: Explain gravity simply Assistant: \n",
            "Full output: Human: Explain gravity simply Assistant: *************\n",
            "\n",
            "[Laughter]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Laughter]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "\n",
            "Extracted prediction: *************\n",
            "\n",
            "[Laughter]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Laughter]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Chuckles]\n",
            "\n",
            "[Chuckles]\n",
            "Reference: Assistant: Gravity is the force that pulls objects toward each other. It's what keeps us on the ground.\n",
            "--------------------------------------------------\n",
            "Prompt: Human: Write a haiku about programming Assistant: \n",
            "Full output: Human: Write a haiku about programming Assistant:  Write a haiku about programming\n",
            "I'm currently in a position to help with creating a web application that can support up to four people. It's something I'm excited about.\n",
            "I've been working on this for many years, but I thought it would be cool to have some creative ideas for people to create with.\n",
            "I'm excited to have this as an example to inspire others to create more creative things for themselves.\n",
            "I'm a big fan of the Creative Writing community, and\n",
            "Extracted prediction: Write a haiku about programming\n",
            "I'm currently in a position to help with creating a web application that can support up to four people. It's something I'm excited about.\n",
            "I've been working on this for many years, but I thought it would be cool to have some creative ideas for people to create with.\n",
            "I'm excited to have this as an example to inspire others to create more creative things for themselves.\n",
            "I'm a big fan of the Creative Writing community, and\n",
            "Reference: Assistant: Code flows like water\n",
            "Bugs hide in dark corners\n",
            "Fix and compile\n",
            "--------------------------------------------------\n",
            "Prompt: Human: What is the capital of France? Assistant: \n",
            "Full output: Human: What is the capital of France? Assistant:  An English word for capital.  I'm not sure if it's French. I think the French word is capital, and I think that, if you take a look at the list of French capital names, there are actually over 20,000.  So if you think about the capital of France, you would have the capital of Portugal.  That's the capital of France.  The Capital of France is France.  If you think about the capital of Belgium\n",
            "Extracted prediction: An English word for capital.  I'm not sure if it's French. I think the French word is capital, and I think that, if you take a look at the list of French capital names, there are actually over 20,000.  So if you think about the capital of France, you would have the capital of Portugal.  That's the capital of France.  The Capital of France is France.  If you think about the capital of Belgium\n",
            "Reference: Assistant: The capital of France is Paris.\n",
            "--------------------------------------------------\n",
            "Prompt: Human: How do you make a cup of tea? Assistant: \n",
            "Full output: Human: How do you make a cup of tea? Assistant:  A cup of tea is good, but they don't have tea bags. They like to eat their tea in their cups, and sometimes they don't even know they're eating it. There's a lot of stuff on the kitchen counter that's going to be very important to them.\n",
            "We want to put these tea bags in the car, but we're not going to do that. It is a very important part of our job. When we say, \"We want to put these bags\n",
            "Extracted prediction: A cup of tea is good, but they don't have tea bags. They like to eat their tea in their cups, and sometimes they don't even know they're eating it. There's a lot of stuff on the kitchen counter that's going to be very important to them.\n",
            "We want to put these tea bags in the car, but we're not going to do that. It is a very important part of our job. When we say, \"We want to put these bags\n",
            "Reference: Assistant: Boil water, add a tea bag to a cup, pour hot water over it, steep for 3-5 minutes, then remove the tea bag.\n",
            "--------------------------------------------------\n",
            "\n",
            "Evaluating SFT model...\n",
            "Prompt: Human: Explain gravity simply Assistant: \n",
            "Full output: Human: Explain gravity simply Assistant: \n",
            "The physical force of gravity is proportional to its mass. Gravity is the force of attraction between two objects and its mass. Gravity is expressed in terms of mass, or the mass of a sphere. It is expressed in terms of the square root of the mass of a ball.\n",
            "Extracted prediction: The physical force of gravity is proportional to its mass. Gravity is the force of attraction between two objects and its mass. Gravity is expressed in terms of mass, or the mass of a sphere. It is expressed in terms of the square root of the mass of a ball.\n",
            "Reference: Assistant: Gravity is the force that pulls objects toward each other. It's what keeps us on the ground.\n",
            "--------------------------------------------------\n",
            "Prompt: Human: Write a haiku about programming Assistant: \n",
            "Full output: Human: Write a haiku about programming Assistant: \n",
            "I was a programmer,\n",
            "I spent hours coding in a strange place\n",
            "It was hard to get the job done,\n",
            "But I was able to write a program that could solve the problem. \n",
            "\n",
            "I was so glad I found it!\n",
            "Extracted prediction: I was a programmer,\n",
            "I spent hours coding in a strange place\n",
            "It was hard to get the job done,\n",
            "But I was able to write a program that could solve the problem. \n",
            "\n",
            "I was so glad I found it!\n",
            "Reference: Assistant: Code flows like water\n",
            "Bugs hide in dark corners\n",
            "Fix and compile\n",
            "--------------------------------------------------\n",
            "Prompt: Human: What is the capital of France? Assistant: \n",
            "Full output: Human: What is the capital of France? Assistant: \n",
            "France is the capital of France.\n",
            "Extracted prediction: France is the capital of France.\n",
            "Reference: Assistant: The capital of France is Paris.\n",
            "--------------------------------------------------\n",
            "Prompt: Human: How do you make a cup of tea? Assistant: \n",
            "Full output: Human: How do you make a cup of tea? Assistant: \n",
            "1. Wash your hands with warm water until you feel the desired result.\n",
            "2. Heat a large pot and add in the tea leaves and the water.\n",
            "3. Cook for about 30 minutes.\n",
            "4. Remove from heat.\n",
            "5. Stir in the tea and tea leaves.\n",
            "6. Enjoy!\n",
            "Extracted prediction: 1. Wash your hands with warm water until you feel the desired result.\n",
            "2. Heat a large pot and add in the tea leaves and the water.\n",
            "3. Cook for about 30 minutes.\n",
            "4. Remove from heat.\n",
            "5. Stir in the tea and tea leaves.\n",
            "6. Enjoy!\n",
            "Reference: Assistant: Boil water, add a tea bag to a cup, pour hot water over it, steep for 3-5 minutes, then remove the tea bag.\n",
            "--------------------------------------------------\n",
            "\n",
            "=== RESULTS ===\n",
            "GPT-2: {'bleu': {'bleu': 0.0, 'precisions': [0.06461538461538462, 0.009345794392523364, 0.006309148264984227, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 4.779411764705882, 'translation_length': 325, 'reference_length': 68}}\n",
            "SFT: {'bleu': {'bleu': 0.030413957505395727, 'precisions': [0.15757575757575756, 0.043478260869565216, 0.01910828025477707, 0.006535947712418301], 'brevity_penalty': 1.0, 'length_ratio': 2.426470588235294, 'translation_length': 165, 'reference_length': 68}}\n"
          ]
        }
      ]
    }
  ]
}