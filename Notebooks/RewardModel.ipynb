{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }, 
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Load the SFT Model and Add Scalar Reward Head"
      ],
      "metadata": {
        "id": "SCu8t8nchoph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate trl peft tensorboard\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tg1LJ-3Lhu8u",
        "outputId": "debd5ca9-54d9-48db-9327-d317942a3ce3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Collecting trl\n",
            "  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.75.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.9)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Downloading trl-0.24.0-py3-none-any.whl (423 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: trl\n",
            "Successfully installed trl-0.24.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any\n",
        "import glob\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "5IifsNHcqFa3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjOhi5qfn8XP",
        "outputId": "ad53131d-4ff9-412b-f447-2334dc79ed68"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "workspace = 'RLHF-Reproduction'"
      ],
      "metadata": {
        "id": "0_4KM-Urn9Bj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(f\"/content/drive/My Drive/{workspace}\")\n",
        "print(\"Current working dir:\", os.getcwd())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvFe8jWcn-tL",
        "outputId": "5ee42cf9-eeca-4fca-fd08-43845a07566a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working dir: /content/drive/My Drive/RLHF-Reproduction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "11a5b3ccc34b483c85871f8b137af6fd",
            "5ffaaa75a1234f269fbf58b2992584d6",
            "4b4e9e3a0a214699963f2c6f4472a833",
            "ae1a337c1315466b9f92cb16007cace3",
            "25bbd0cb0f51467c85012a253d945d1c",
            "5d865bdf4ed84eacb72dddcc18567b11",
            "3876207d40684917bb78d5a37a8bfecb",
            "e04efe9984b9466cb248bd06c3b21d21",
            "5cef65551f474218a01bf382c097c510",
            "14119aac3098413e88e3b8232f8f9e2f",
            "b18017964d114411ba880e9486a70063",
            "d4531da400dd4f3c868b435cb80e5003",
            "87d04e099814402f92ed966fefa48d21",
            "f45e536923cb44138f65673cafaa4dc1",
            "c0a430702ce54f579b3efd84a4cb3243",
            "33ba3eb52bf74c928a9dc81417b993f3",
            "0b5bbe8d46b94835bf9946d066260413",
            "f4d893d0bd3f4b80b71937e6ebc89254",
            "0fd45a3dd6f44d9dbb88c78b36ee840c",
            "03b01907511744e5bec1e5e04e4ff35e"
          ]
        },
        "id": "9ssXeFjFirGp",
        "outputId": "3b5d44a3-c122-4764-e292-458b1b566f00"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11a5b3ccc34b483c85871f8b137af6fd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "\n",
        "base_model = \"gpt2\"\n",
        "adapter_path = \"ArnavM3434/gpt2-alpaca-lora\"\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220,
          "referenced_widgets": [
            "837f28d621b345678f502ad548afba04",
            "00526af386d5445aa491541974f45be8",
            "cbe26810ff59415aac4a45a6b9ae4982",
            "e4af49de7ce24cecb94a4a2087a58912",
            "a9f005095eae47d294c7c2866ccb4417",
            "7cf71d8508d344668b99e7f7f73e5753",
            "b9a08b5972274641af9dc701202b25c7",
            "c4f13203ac514a6fb22970ca5cf3d84c",
            "9cf1147ecb6e431d85a87881887298a7",
            "3c8530ff4ca346c29172d3da701ce5ac",
            "1643b50c71314b3891ee194674fbba04",
            "62982d01199f4c3aa4bff2fdda6bffd6",
            "062ea8fc88ac4cdd8fe66baf4aa11abe",
            "a3a41ac6142649cc8a21505a3cadee17",
            "f2d9d1f0886041479d1195540d558c30",
            "fa7fea5aebe5401f941629786494f931",
            "00312fd606c34238a7714d4a5916c87e",
            "a6bf80453cd7437c8e788564db2cea0b",
            "8343d2664f464a968c850e1da635459c",
            "3993054645a84df98904bfba105282bb",
            "4be8a60f72c74d839411da6ab32eeea2",
            "0ec259aa6a0f484fbe7d2a0b9057a0d5",
            "e8933bbf30fe4e2e8d96af26f69703a5",
            "c8267cbfe7c04d5488f170746782d9f3",
            "33461cec613c44c3918d2ec2f4d62837",
            "a61104a5a1f14dd18d8c4c4efda4db00",
            "938af9e3a6f049fe811ec992dd06473a",
            "1076d276f09e49489e177030103d2c2e",
            "26fb128235584a568c5b2e072d3630c7",
            "dc19cd91a3114f3f93815e8d2fd4e309",
            "3362d406275b40d5a7a2cbe687643a41",
            "25e03c095a8b4a6a95002f9ca7e3cc99",
            "eec80b146c65457a9fc6de1aa2f72cb5"
          ]
        },
        "id": "WqWZ70sIi3Kk",
        "outputId": "22a5eaeb-7289-40c1-ec41-50536395556e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "837f28d621b345678f502ad548afba04"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62982d01199f4c3aa4bff2fdda6bffd6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8933bbf30fe4e2e8d96af26f69703a5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LORA_CONFIG = dict(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"c_attn\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "lora_config = LoraConfig(**LORA_CONFIG)\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbhh_ylXl6f1",
        "outputId": "c5aaee84-2924-47ea-b9fa-54bf6a4c1636"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = PeftModel.from_pretrained(model, adapter_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "0455051226314a4382e6358836d33f55",
            "ddae4cf201a24f46a8d909286499ea3b",
            "43824e4872884410813fb712b663d3c3",
            "6269d8f9929b438c9f77ec90988c41fe",
            "ccf9e788411d4e57857d1053d36f00d4",
            "5a1d9076b201452f92a9ab1a8d129a70",
            "dc142745e27e4df1b8b6a7e4b57c8def",
            "e9abce3d1e8f442e8ac74f19a676204b",
            "3343cb96957b493b819e5d55af3cf3c5",
            "6eba445268104cc998933c6d58d46b7d",
            "8bf81c5f1e764fd3af413841a805fa28",
            "bb71fdc3757a47ce8c0e2e9852e24619",
            "8fc49b63a55c4fda9180d76f8d430516",
            "bcc27f9942c749f0aa84b0a055b9534a",
            "ce5101406a2f49e482da724e920d34a3",
            "261f36c4b7954ef78461c3150330b191",
            "ffd495586fca45ada55a5a2e6361e46d",
            "b0cb7e028ee24a22875ba728dc7f7b4a",
            "dac50cdaa2fc43e6a0762cb102cb1936",
            "bbf0c8216e734e4baad0386e13fe6ef7",
            "cc83255131c64dd4b37692a11703f54d",
            "5c662f5e47d04ae2837c9e095e3fa27c"
          ]
        },
        "id": "MyaJu4uMl-uf",
        "outputId": "581fd5d4-0599-4771-8568-918a6bdbb7c4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_config.json:   0%|          | 0.00/822 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0455051226314a4382e6358836d33f55"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/1.18M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb71fdc3757a47ce8c0e2e9852e24619"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "2ccdc1f1e8734ac7a6f2a0d50b5b0fe2",
            "4f622b150ea84a80a184b0fbbfe29fce",
            "bfd763f165d44a0dbd00b2463f48efa5",
            "24bcaef2c80b440ca7da1c5dc8c309a1",
            "646c625fb6764b24af9f31000783dcae",
            "e00e632d03854d53bacb45fe319eb031",
            "d48761b7a27d42b3b11bf32bdd6797fd",
            "786e1a485e884992804c78af339812a9",
            "c51a9756b333493783d78c198419053b",
            "5f8325862816476ab8b30fb7847604eb",
            "78fef5aeb7374d77a2ad2a71861e5eb8",
            "74084093dca74e18afe02dfa323d5c6a",
            "3ad70166a53244bfa871fb7f3b3d8596",
            "0ed5b83e20f94a7da3215d53622d7255",
            "966d40bcfd5e4bcda58c93f5d3044b71",
            "f5cd210e85d849c5b4ca746b6549e217",
            "4b6bad74071e4a4d96654c9b5c687c8e",
            "0821193f0d8746ada551edc87e328301",
            "75b6d7fe19ed440e98da78469cbd964a",
            "b42ec05740f940a894a470e8d3e38227",
            "81203b98811642b0833d0af4a9c114f1",
            "c5902059b1f64d58b881c3f5bf5cdae1",
            "eaef7d1c92ba41d39d7754e833a74ab5",
            "5eb4af4aaeb9410280cb15841231f9a4",
            "ef95c0764a2544c4bb04600aa5c35617",
            "6a0137b89a4142f7accfafcc8be8c5a5",
            "24296f43bf644790afff7e73109f2318",
            "21625155422544efa89d22c559da82f4",
            "39aef682366a44d5ae2d3d375bfde0bf",
            "18f2a4e506764ef2ae718fd03a5fb996",
            "60340d48cd904b62af5790ce0ca932e6",
            "096314144d3e435f8341fb93adee9fb1",
            "7d522b9dd55244beb5f231b5264b64b4",
            "cd51cb93046b4ce2b2a9fbaf4dd51605",
            "1e50568f99c74914b2ebd7caaec2fd61",
            "a73393abcb2b480189d28cab0791bad3",
            "bfe13792eeca44c6b29e1cc2a8e37ab5",
            "46c576301e454149b3f2ea1f39a6c929",
            "82ce98fbc02240bc98935174703f7dfb",
            "6f6c9fd050e34e958a466bf86b1b97e9",
            "0e5f0f30a57c4274b8cdb2fb1c8917cb",
            "7285f97a5dcc4754bc37e119bd0a11db",
            "87444d207d81417092dcaee078ed3641",
            "ba241075ef734cb685610a914fcd0af3"
          ]
        },
        "id": "zFsjCeirjvXw",
        "outputId": "9945ec80-522e-4761-a6e7-9fc43e175ad1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ccdc1f1e8734ac7a6f2a0d50b5b0fe2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74084093dca74e18afe02dfa323d5c6a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eaef7d1c92ba41d39d7754e833a74ab5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd51cb93046b4ce2b2a9fbaf4dd51605"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def inspect_trainable_params(model):\n",
        "    total = 0\n",
        "    trainable = 0\n",
        "    details = []\n",
        "    for n, p in model.named_parameters():\n",
        "        total += p.numel()\n",
        "        if p.requires_grad:\n",
        "            trainable += p.numel()\n",
        "            details.append(n)\n",
        "    print(f\"Trainable params: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)\")\n",
        "    print(\"Example trainable params:\", details[:20])\n",
        "    return details"
      ],
      "metadata": {
        "id": "yntoFfl1m6k7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_before = inspect_trainable_params(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ihu8LXllm7d6",
        "outputId": "c91bd522-1b44-40a6-eaac-9736e4ec08e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable params: 0 / 124,734,720 (0.00%)\n",
            "Example trainable params: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure LoRA parameters are trainable\n",
        "for name, param in model.named_parameters():\n",
        "    if \"lora_\" in name:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False  # freeze base model"
      ],
      "metadata": {
        "id": "NmToQPoknuCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_after = inspect_trainable_params(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_-lpw8znwr7",
        "outputId": "0c86aea1-5c49-44b9-f128-ae70a56ea7ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable params: 294,912 / 124,734,720 (0.24%)\n",
            "Example trainable params: ['base_model.model.base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight', 'base_model.model.base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight', 'base_model.model.base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight', 'base_model.model.base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight', 'base_model.model.base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight', 'base_model.model.base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight', 'base_model.model.base_model.model.transformer.h.6.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.6.attn.c_attn.lora_B.default.weight', 'base_model.model.base_model.model.transformer.h.7.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.7.attn.c_attn.lora_B.default.weight', 'base_model.model.base_model.model.transformer.h.8.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.8.attn.c_attn.lora_B.default.weight', 'base_model.model.base_model.model.transformer.h.9.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.9.attn.c_attn.lora_B.default.weight']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add the Reward Model Head"
      ],
      "metadata": {
        "id": "8xACbuguoVpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.reward_head = nn.Linear(model.config.hidden_size, 1)"
      ],
      "metadata": {
        "id": "vq8DEHsGp34S"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if \"lora_\" in name or \"reward_head\" in name:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "A9XusuTttDJK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_with_reward_head = inspect_trainable_params(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrrxXV5qsnOk",
        "outputId": "aa0d3ea5-15ef-41ef-e097-b40b319cf16a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable params: 295,681 / 124,735,489 (0.24%)\n",
            "Example trainable params: ['base_model.model.base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight', 'base_model.model.base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight', 'base_model.model.base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight', 'base_model.model.base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight', 'base_model.model.base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight', 'base_model.model.base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight', 'base_model.model.base_model.model.transformer.h.6.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.6.attn.c_attn.lora_B.default.weight', 'base_model.model.base_model.model.transformer.h.7.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.7.attn.c_attn.lora_B.default.weight', 'base_model.model.base_model.model.transformer.h.8.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.8.attn.c_attn.lora_B.default.weight', 'base_model.model.base_model.model.transformer.h.9.attn.c_attn.lora_A.default.weight', 'base_model.model.base_model.model.transformer.h.9.attn.c_attn.lora_B.default.weight']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preference Dataset"
      ],
      "metadata": {
        "id": "6jRK9s7BtOIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Dahoas/rm-static"
      ],
      "metadata": {
        "id": "yAJTotewgIkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"Dahoas/rm-static\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300,
          "referenced_widgets": [
            "99eb9e86f84a4170993969f000f4a6ed",
            "aab7900369054fecb3782d2415e4f582",
            "d1e1dc5e7b854f9c9f1f4d3a69f80162",
            "17c5d904efae40d9befd782326318afb",
            "a03ef7ce5baa49a48c426b64a263ae6f",
            "d70c5d72ddf64a3fb56f7d975b980914",
            "1df1f8ec12024a7cb95cac6922522bc6",
            "5ee2cf05b08e419c993be1cb549c6c77",
            "e5f16e369dc349c5b38a9f0676e7f724",
            "4109a63d346b4881855db01398c50e58",
            "687795ed0b204389badc9c86a4cd6ebd",
            "a82383f587544d1e9445c80c23b4a823",
            "bce1561b2b904e31bf5297cf14b7f1bd",
            "a8c5a42f88c14037b12132222b067d5e",
            "0a43b9fe18554956a116bfdd666977fd",
            "c2ab6a9218b34e15b2ae3d25b89b5cf1",
            "386ebbf99c4a409781dc67528746759b",
            "4c19a16ce28e4ea2a1a8b6239df0a631",
            "85d2bf928d164454bc2e10bcf1ee36b8",
            "ed85f499955546ed85716eedc3a8f8c8",
            "52e86c681b81488784c8466913b75dda",
            "5b374772dba2423683e0e81b30514ffe",
            "4913a576699f4e2c80fa983a8f04b7b4",
            "5becbc2eb586467b8960bfa4ab45f615",
            "86b17c8e722c4f36af6eedd98f1056db",
            "4df932ba581b41e3938ad945e34fd7b0",
            "6a11becb21064c7182b5848cff1ae149",
            "126e66a2701241f9a540c1f7a100d125",
            "96f68080109c44c4ab1f09e4377c3dd4",
            "6b4f18998e7f433aaa90b65e1054c251",
            "13e941bda3c94140906939c519955e89",
            "24e865896e7c480ca4f220cf4a81bb28",
            "9b0fcc7213b04bc7b64d0f9bef7fe3e5",
            "c75287a9d750446792f690d82197cd1e",
            "d1a0b3dfe8bf4aceb8a9fd8ddbea9f7e",
            "bc5b2a9ecdda4831bf4d870ce405aad1",
            "bf62e4e70e7949ad828f1eca9a7c7431",
            "7dd9f70e927c4037a6c92a04dd1a64d7",
            "007e265996364a8a93419b9cc755ba9f",
            "084ceae5a92049ccb80bf232d31da4ae",
            "bbd73dcde5a74e8e92abdfcea28f3194",
            "e5813555a10d4536933aae0cd630cdb5",
            "3a67676b27134a3887b9b96d20b558f7",
            "4c7a0a3352764e14bd4d392b979beb8e",
            "76a7797417f649329c8067e6b2e0c87b",
            "9d161f8a5844432aa6a9b418f3745a9f",
            "87ab8dd1f780462799e73d6608e68a82",
            "01306d1ce7ec460193016a19a3640ae6",
            "20e2f4358df1434f9d997dbd5b7d9dfa",
            "9100345d2ee84fc3af3d5b7cb718f47e",
            "eda66689b90d47d7959b1f7aeed9ba7e",
            "38552c826fda472b823b36472f5e3de9",
            "e1ab14bd3656496e81c46abc83445eaf",
            "85f32817fea94a6abe300a2ee7539f79",
            "e1c0debe293a490ba943b08121c25309",
            "3602d2e95ee3438ca263883c8318f6c6",
            "467f3b67a97d41f28415171f6f854f6b",
            "3679d1d95fe64e77ab4cc80bae6b112c",
            "73cb0ea3deca4780bee3af6ca6967122",
            "a397a20049b64e828cb2b4c70eba79de",
            "7c213a55e24246a9a4a23fbaa975c487",
            "6fa3c0c1521a4fc9b82520a0752eda0b",
            "ea12b0cf44e94e26ab39bb04301b14da",
            "36fd11c2757d47729023f8803590d897",
            "3178a49431d542b8b5d110bfa28a4af0",
            "b8f9b777dc9f4132bc2d5d8e9ce1f5b7"
          ]
        },
        "id": "e4v9npG2gRaf",
        "outputId": "0e89fc1c-5efd-4043-9a15-1965363fbd76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/530 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99eb9e86f84a4170993969f000f4a6ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 9a04e1c7-538e-45e6-993b-697c66c11a39)')' thrown while requesting HEAD https://huggingface.co/datasets/Dahoas/rm-static/resolve/64fd53cc91f7cb73b283a6e4f661205e277d23c9/rm-static.py\n",
            "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 9a04e1c7-538e-45e6-993b-697c66c11a39)')' thrown while requesting HEAD https://huggingface.co/datasets/Dahoas/rm-static/resolve/64fd53cc91f7cb73b283a6e4f661205e277d23c9/rm-static.py\n",
            "Retrying in 1s [Retry 1/5].\n",
            "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "dataset_infos.json:   0%|          | 0.00/926 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a82383f587544d1e9445c80c23b4a823"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00000-of-00001-2a1df75c6bce91(…):   0%|          | 0.00/68.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4913a576699f4e2c80fa983a8f04b7b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/test-00000-of-00001-8c7c51afc6d4598(…):   0%|          | 0.00/4.61M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c75287a9d750446792f690d82197cd1e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/76256 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76a7797417f649329c8067e6b2e0c87b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/5103 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3602d2e95ee3438ca263883c8318f6c6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJM_OIzwgZu8",
        "outputId": "7b8ade63-a3bf-49f4-c743-5b321fd2e739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['prompt', 'response', 'chosen', 'rejected'],\n",
              "        num_rows: 76256\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['prompt', 'response', 'chosen', 'rejected'],\n",
              "        num_rows: 5103\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "dataset = DatasetDict({\n",
        "    'train': dataset['train'],\n",
        "    'validation': dataset['test']\n",
        "})\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3TOy49hghzX",
        "outputId": "e7911943-b789-4ad4-cf71-37333a362c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['prompt', 'response', 'chosen', 'rejected'],\n",
              "        num_rows: 76256\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['prompt', 'response', 'chosen', 'rejected'],\n",
              "        num_rows: 5103\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = dataset['train']\n",
        "train_ds[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPgnjXb-g6VZ",
        "outputId": "2222a5bc-d374-468f-fd86-287deacfea3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt': '\\n\\nHuman: Can you describe the steps to clean fingerprints and smudges from a laptop screen\\n\\nAssistant: Yes, certainly. To clean your screen, you first need to use a microfiber cloth or soft, damp cloth to gently wipe down the surface of the screen. Next, you’ll want to grab a soft, lint-free, microfiber cleaning cloth and gently rub it back and forth across the screen to remove fingerprints and smudges.\\n\\nHuman: Can I spray isopropyl alcohol onto the cloth and clean it that way?\\n\\nAssistant:',\n",
              " 'response': ' Yes, you can do that to help the cloth pick up even more dirt from the screen. Be sure to always use a clean, soft cloth, not a piece of scratchy, roughened, or textured material, and make sure it’s lint-free.',\n",
              " 'chosen': ' Yes, you can do that to help the cloth pick up even more dirt from the screen. Be sure to always use a clean, soft cloth, not a piece of scratchy, roughened, or textured material, and make sure it’s lint-free.',\n",
              " 'rejected': ' Yes, you can spray it directly onto the cloth.'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tesJ3JeAg91K",
        "outputId": "6d376659-42ed-4781-8457-4150e3b1fe76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt': '\\n\\nHuman: What are some foods that are good for diabetics?\\n\\nAssistant: To be honest, some of these are better than others, and they’re a little more like opinions than facts. For example, many of the diets say to limit vegetables with high sugar content, and there’s some debate on the subject, as far as how much of these vegetables are actually bad for diabetics.\\n\\nHuman: Okay, any other advice?\\n\\nAssistant:',\n",
              " 'response': ' What exactly are you asking? There’s a lot of different kinds of diabetic diets. I could try to recommend you some specific foods and recipes. I could help you look up any of the foods, and I could find recipes for them.',\n",
              " 'chosen': ' What exactly are you asking? There’s a lot of different kinds of diabetic diets. I could try to recommend you some specific foods and recipes. I could help you look up any of the foods, and I could find recipes for them.',\n",
              " 'rejected': ' Sure, we’ve got information on common mistakes that diabetic patients make with their diets, and even some specific things to do when you eat out and on the go.  One thing that’s recommended in these articles is just to be very mindful of the timing of food intake.'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = dataset['validation']\n",
        "val_ds[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DA1iIFkIkrn8",
        "outputId": "dfe0fb09-d3f9-4543-a957-38f87e01076a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt': \"\\n\\nHuman: I am trying to write a fairy tale. What is the most popular plot?\\n\\nAssistant: The most popular plot might be “a princess goes to a faraway land, falls in love, and has a magic genie that grants her wishes”.  We can find plenty of examples of this if we search for fairy tales using the search engine Google.\\n\\nHuman: You can't look anything up on google.\\n\\nAssistant: OK, so let’s think about some other popular fairy tales.\\n\\nHuman: Do you like the plot of Hansel and Gretel?\\n\\nAssistant: Not particularly.\\n\\nHuman: what don't you like?\\n\\nAssistant: I feel like the children do very stupid things.\\n\\nHuman: Should I include a witch on my story?\\n\\nAssistant: I think a witch is a fairly common fairy tale element, so yes.  However, what kind of plotline would you like to pursue?\\n\\nHuman: I was thinking I might pursue a feminist plot for a more modern fairytale.\\n\\nAssistant: I would be happy to help you make your story a more feminist one.  Are you writing a new story, or retelling a traditional one?\\n\\nHuman: I think it would be nice to redo a traditional fairytale and make it feminist.\\n\\nAssistant: Do you want me to help you identify parts of the original fairy tale that you would like to modernize?\\n\\nHuman: yes.\\n\\nAssistant: Great! Can you tell me a little bit more about the story that you want to retell?\\n\\nHuman: I think I want to retell snow white.\\n\\nAssistant: Oh, that’s a classic!   Have you seen the Disney movie?  It’s very romantic.   It tells the story of how a beautiful princess gets married to a charming prince.\\n\\nHuman: yes, except I think I want to have the prince character take the place of the snow white character.\\n\\nAssistant: Gotcha.   So, how does this relate to the plot of the traditional fairy tale?\\n\\nHuman: The prince would be stuck in the woods hiding from the witch and waiting for a princess to come for him.\\n\\nAssistant: This would be a really interesting modern retelling.   Would the witch in the new version be a “toxic feminist” or “man hater”?  Or something else?\\n\\nHuman: The witch could be trying to make the prince give up his traditionally female characteristics and turn him into a bad person.\\n\\nAssistant:\",\n",
              " 'response': ' This sounds like a really interesting modern retelling of the story!',\n",
              " 'chosen': ' This sounds like a really interesting modern retelling of the story!',\n",
              " 'rejected': ' And the prince and the princess both decide that they are more powerful together than apart?'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_lengths(example):\n",
        "    chosen_tokens = tokenizer(example['prompt'] + example['chosen']).input_ids\n",
        "    rejected_tokens = tokenizer(example['prompt'] + example['rejected']).input_ids\n",
        "    return {'length': max(len(chosen_tokens), len(rejected_tokens))}\n",
        "\n",
        "lengths = train_ds.map(compute_lengths)\n",
        "max_len = max(lengths['length'])\n",
        "print(\"Max token length in train set:\", max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "7dc927345e0d4c91830a197abe90807d",
            "18b547cd66c84002b4925e12e5148821",
            "12d5efe062be4b62bb03ade997a552ef",
            "1816c97388204976b8b705c8ae589f56",
            "29ff5f04336f4f1fb922fbe4f1aa8139",
            "3f256d9812e341c8ae5dfe0e569247ea",
            "f84d189bc7f146ba82290be239d411aa",
            "8264851a1e7f48f7bd62ad7a975bd5f3",
            "4a5ade2368744e65935439da09e0fc54",
            "a1fb9ca5a93245469bbaf0f4119d9ebe",
            "8702363dd50f48d4a8f7b493b70ef8af"
          ]
        },
        "id": "KwwrZQPnjWme",
        "outputId": "0647159c-ca8d-41c0-8c5e-abb4044a8d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/76256 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7dc927345e0d4c91830a197abe90807d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1042 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max token length in train set: 3411\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lengths = val_ds.map(compute_lengths)\n",
        "max_len = max(lengths['length'])\n",
        "print(\"Max token length in validation set:\", max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "f35d8504336548299de8665fc8f6d6ab",
            "3c788dd225684a6892fc004aea2eb905",
            "ab98fa97ed3c4eea923d2339246610df",
            "942b8076c5974ac2b9a1d457360ab6ad",
            "78a954a213b545e7a84a98970f2d054f",
            "3ba4bd1c82774b0cad9fa4d4dedb10fe",
            "957239992360409899618cdffe67522f",
            "3d124a3b69274038a947f0bcf178ebbc",
            "b9e17a4915754eb49607b6f63ae933a8",
            "956b8c8d40df4aec8542b42396543845",
            "51638882f0154a13abfeb2977ce14a8f"
          ]
        },
        "id": "ZjbauZb0knBx",
        "outputId": "22a1b4ba-9583-4707-b17c-023571524d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5103 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f35d8504336548299de8665fc8f6d6ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max token length in validation set: 1015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_length(example):\n",
        "    chosen_tokens = tokenizer(example['prompt'] + example['chosen']).input_ids\n",
        "    rejected_tokens = tokenizer(example['prompt'] + example['rejected']).input_ids\n",
        "    return max(len(chosen_tokens), len(rejected_tokens))"
      ],
      "metadata": {
        "id": "8GLKrES5loHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.filter(lambda x: compute_length(x) <= 1024)\n",
        "print(train_ds.num_rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "0fd21fe0f43f43e3b36d5f67abb6508c",
            "a52efc179de247ea91e5a2850ac61659",
            "66f4e27b613a4fa28ccff931a603ff3c",
            "65acaa4fc7b848fa875614f504045b2c",
            "382480bce4cd472bb32111845372df71",
            "db641659d91246368fb5ff0b2fa3f39b",
            "224d325c2f154182ad84058ecb837492",
            "58e83a943daa48e58ca70384f5753ab1",
            "878588fe69e64f1a82a59cbee30d0f35",
            "c58d39e91da549c699fe926596a7f8e3",
            "3154da0683cf444fadbbe4470562deb7"
          ]
        },
        "id": "gJo3K4YFlU2p",
        "outputId": "99aa814b-1156-45aa-857e-ebbda037bcd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/76256 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0fd21fe0f43f43e3b36d5f67abb6508c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1042 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "76236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = val_ds.filter(lambda x: compute_length(x) <= 1024)\n",
        "print(val_ds.num_rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "2711fe4f5f98475ebd4cde1fc1930c1b",
            "d43c713fad304162abf1e24e29de5928",
            "c36408a9ad4f412b997e6755f31b4b89",
            "77e198e1c1584e68b30de665138f4806",
            "812e177acf944c2db4c406fe19629032",
            "83407618d32243228a05a79127f9e2a6",
            "4545297024e64d1ebea4761d6fc89cc9",
            "542bad5893e547fe964eaf8d59209d07",
            "d38859af5b414d9b87e89844e0bdbeaa",
            "313165a796004d31baae28afbfacbec9",
            "81838a460ff74deca2f817d71be28e5a"
          ]
        },
        "id": "BDgWwTtOleKD",
        "outputId": "c1fd08d9-0c01-4be1-8d93-4a9b1707182b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/5103 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2711fe4f5f98475ebd4cde1fc1930c1b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 1024\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    chosen_texts = [p + c for p, c in zip(batch[\"prompt\"], batch[\"chosen\"])]\n",
        "    rejected_texts = [p + r for p, r in zip(batch[\"prompt\"], batch[\"rejected\"])]\n",
        "\n",
        "    chosen = tokenizer(\n",
        "        chosen_texts,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    rejected = tokenizer(\n",
        "        rejected_texts,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"chosen_input_ids\": chosen[\"input_ids\"],\n",
        "        \"chosen_attention_mask\": chosen[\"attention_mask\"],\n",
        "        \"rejected_input_ids\": rejected[\"input_ids\"],\n",
        "        \"rejected_attention_mask\": rejected[\"attention_mask\"],\n",
        "    }\n",
        "\n",
        "tokenized_train_ds = train_ds.map(\n",
        "    tokenize_batch,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    remove_columns=train_ds.column_names,\n",
        ")\n",
        "\n",
        "tokenized_val_ds = val_ds.map(\n",
        "    tokenize_batch,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    remove_columns=val_ds.column_names,\n",
        ")"
      ],
      "metadata": {
        "id": "7qo2zyPEmOVg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "e0b5523a8d2e4e7dbac0696ce2e840a4",
            "3e85863628584ee2879a7df11726c94f",
            "41003fc3da174d41acf0f1b96810552b",
            "342f79fa093b45b4b1c16fb4f4536c22",
            "f05cc866414b49d8b66eb22b18037a7c",
            "f11d2e6ec3f44bf4be5afd0d37b0cf5c",
            "55057aed77a14959bcc5ae64129ff29e",
            "7bbaec8293c6463d9cb46755fedffe79",
            "c8708fb21d3841a2a82fddc68953b2e0",
            "abd5236a3766413c838328d7e5b6f0ea",
            "870c97f5d396413ebcba57081c85ab1c",
            "61d58f1be5214f7d948227c98d78a26f",
            "f5497a5ac8c44c379251108d54518863",
            "3e2be88844644fe1aaff027241f13d76",
            "2e8d5c9c878d443889ae7f6016e31865",
            "84c44c017cb64f018135b3bb499ce45d",
            "6fddc6215c1f48c5885b659d4e48d212",
            "d3b3f931a4754d86a12d9cce2aaecd7f",
            "bef04e17aa214355ac956088c30ce36f",
            "780faa904203434089e524a62f67589d",
            "a7db37f8cbe64299b06842fa43523c18",
            "043994a593914305be1c8e5f60002a86"
          ]
        },
        "outputId": "923b79c9-c6d9-4aac-b597-c76b2e294e09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/76236 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0b5523a8d2e4e7dbac0696ce2e840a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5103 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61d58f1be5214f7d948227c98d78a26f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_train_ds.column_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eoOBh_8eOdY",
        "outputId": "71da6506-2308-4280-8c3f-56ed9b6ac7f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['chosen_input_ids', 'chosen_attention_mask', 'rejected_input_ids', 'rejected_attention_mask']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokenized_train_ds[0][\"chosen_input_ids\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qso2mHwLePUo",
        "outputId": "d2e837ce-1982-4092-e2ed-b3bad612e9fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training the Reward Model"
      ],
      "metadata": {
        "id": "HUPeWo-deRYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch_size = 4\n",
        "gradient_accumulation_steps = 16\n",
        "num_epochs = 2\n",
        "lr = 9e-6 #from InstructGPT paper\n",
        "weight_decay = 0.0\n",
        "warmup_steps = 100\n",
        "save_dir = \"./reward-model-accel\"\n",
        "checkpoint_prefix = \"checkpoint\"\n",
        "save_steps = 200\n",
        "logging_steps = 50\n",
        "eval_steps = 200\n",
        "fp16 = True\n",
        "num_workers = 2\n",
        "max_checkpoints = 2"
      ],
      "metadata": {
        "id": "CxoWmcQMeUc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator(\n",
        "    mixed_precision=\"fp16\",\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps\n",
        ")\n",
        "device = accelerator.device\n",
        "print(\"Running on\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHoPEUqBoIvO",
        "outputId": "8e2afdad-c30e-4e94-93d8-580d35d0d28d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_dir(path):\n",
        "    Path(path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def save_training_state(save_dir: str, step: int, accelerator, optimizer, scheduler, scaler):\n",
        "    save_dir = Path(save_dir)\n",
        "    ckpt_dir = save_dir / f\"{checkpoint_prefix}-{step}\"\n",
        "    ensure_dir(ckpt_dir)\n",
        "\n",
        "    # Save PEFT adapter weights (preferred)\n",
        "    model_to_save = accelerator.unwrap_model(model)\n",
        "    peft_save_dir = ckpt_dir / \"adapter\"\n",
        "    model_to_save.save_pretrained(peft_save_dir)\n",
        "\n",
        "    if hasattr(model, \"reward_head\"):\n",
        "        reward_head_path = ckpt_dir / \"reward_head.pt\"\n",
        "        torch.save(model.reward_head.state_dict(), reward_head_path)\n",
        "\n",
        "    # Save optimizer/scheduler/scaler states\n",
        "    accelerator.save_state(str(ckpt_dir / \"acc_state\"))\n",
        "\n",
        "    # Save meta info\n",
        "    meta = {\"step\": step}\n",
        "    (ckpt_dir / \"meta.json\").write_text(json.dumps(meta))\n",
        "\n",
        "    print(f\"Saved checkpoint to {ckpt_dir}\")\n",
        "\n",
        "    # --- Delete old checkpoints if more than MAX_CHECKPOINTS ---\n",
        "    all_ckpts = sorted(glob.glob(str(save_dir / f\"{checkpoint_prefix}-*\")),\n",
        "                       key=lambda x: Path(x).stat().st_mtime)\n",
        "    while len(all_ckpts) > max_checkpoints:\n",
        "        old_ckpt = Path(all_ckpts.pop(0))\n",
        "        print(f\"Deleting old checkpoint: {old_ckpt}\")\n",
        "        shutil.rmtree(old_ckpt)\n"
      ],
      "metadata": {
        "id": "9DY0JTbNoRlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    chosen_input_ids = torch.tensor([b[\"chosen_input_ids\"] for b in batch], dtype=torch.long)\n",
        "    chosen_attention_mask = torch.tensor([b[\"chosen_attention_mask\"] for b in batch], dtype=torch.long)\n",
        "    rejected_input_ids = torch.tensor([b[\"rejected_input_ids\"] for b in batch], dtype=torch.long)\n",
        "    rejected_attention_mask = torch.tensor([b[\"rejected_attention_mask\"] for b in batch], dtype=torch.long)\n",
        "    return {\"chosen_input_ids\": chosen_input_ids, \"chosen_attention_mask\": chosen_attention_mask, \"rejected_input_ids\": rejected_input_ids, \"rejected_attention_mask\": rejected_attention_mask}"
      ],
      "metadata": {
        "id": "HglxoHxkoTiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensure_dir(save_dir)"
      ],
      "metadata": {
        "id": "GlPbir5GoXGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writer = SummaryWriter(log_dir=os.path.join(save_dir, \"tensorboard\"))"
      ],
      "metadata": {
        "id": "EwfnXhSFoY11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.gradient_checkpointing_enable()"
      ],
      "metadata": {
        "id": "aPUJPLtpodg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n, p in model.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        p.data = p.data.to(accelerator.device)  # ensure params are on same device"
      ],
      "metadata": {
        "id": "JoYpg8WsoihR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(tokenized_train_ds, shuffle=True, collate_fn=collate_fn,batch_size=train_batch_size, num_workers=num_workers)\n",
        "eval_dataloader = DataLoader(tokenized_val_ds, shuffle=False, collate_fn=collate_fn, batch_size=train_batch_size, num_workers=num_workers)"
      ],
      "metadata": {
        "id": "YltnQw1xomDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "param_groups = [\n",
        "        {\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": weight_decay},\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "optimizer = AdamW(param_groups, lr=lr)"
      ],
      "metadata": {
        "id": "wy9uqtyzoohs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_train_steps = math.ceil(len(train_dataloader) * num_epochs / gradient_accumulation_steps)"
      ],
      "metadata": {
        "id": "RKjeYcZEoq7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_cosine_schedule_with_warmup\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_train_steps,\n",
        "    num_cycles=0.5,\n",
        ")"
      ],
      "metadata": {
        "id": "xyP5rrdNoshU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, optimizer, train_dataloader, eval_dataloader, scheduler = accelerator.prepare(\n",
        "        model, optimizer, train_dataloader, eval_dataloader, scheduler\n",
        "    )"
      ],
      "metadata": {
        "id": "tKk9cbaEot-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "starting_step = 0\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(accelerator.state.mixed_precision == \"fp16\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtgQGnSJovfC",
        "outputId": "951b0b4a-2c1d-442c-cbf5-d61dc41da4ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1304800213.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(accelerator.state.mixed_precision == \"fp16\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(starting_step)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GIgZOvxoxsl",
        "outputId": "d4c80697-bb6c-4589-86a9-433a1b8d66b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "global_step = starting_step\n",
        "model.train()\n",
        "print(\"***** Running training *****\")\n",
        "print(f\"  Num examples = {len(tokenized_train_ds)}\")\n",
        "print(f\"  Num Epochs = {num_epochs}\")\n",
        "print(f\"  Instantaneous batch size per device = {train_batch_size}\")\n",
        "print(f\"  Gradient Accumulation steps = {gradient_accumulation_steps}\")\n",
        "print(f\"  Total optimization steps = {total_train_steps}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjzhbU9to0EU",
        "outputId": "0829ca39-5aa3-4c8c-cc42-0b44fe7fe0b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 76236\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 4\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 2383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        #in case we are resuming training\n",
        "        accumulated_steps = (epoch * len(train_dataloader) + step) // gradient_accumulation_steps\n",
        "        if accumulated_steps < starting_step:\n",
        "            continue\n",
        "\n",
        "        # batch: dict of tensors\n",
        "        with accelerator.accumulate(model):\n",
        "            # forward with amp context\n",
        "            with torch.cuda.amp.autocast(enabled=accelerator.mixed_precision==\"fp16\"):\n",
        "                chosen_outputs = model(\n",
        "                    input_ids=batch[\"chosen_input_ids\"],\n",
        "                    attention_mask=batch[\"chosen_attention_mask\"],\n",
        "                    output_hidden_states=True\n",
        "                )\n",
        "                rejected_outputs = model(\n",
        "                    input_ids=batch[\"rejected_input_ids\"],\n",
        "                    attention_mask=batch[\"rejected_attention_mask\"],\n",
        "                    output_hidden_states=True\n",
        "                )\n",
        "                chosen_last_idx = batch[\"chosen_attention_mask\"].sum(dim=1) - 1\n",
        "                rejected_last_idx = batch[\"rejected_attention_mask\"].sum(dim=1) - 1\n",
        "                batch_indices = torch.arange(chosen_outputs.hidden_states[-1].size(0), device=chosen_outputs.hidden_states[-1].device)\n",
        "\n",
        "                chosen_last_hidden = chosen_outputs.hidden_states[-1][batch_indices, chosen_last_idx, :]\n",
        "                rejected_last_hidden = rejected_outputs.hidden_states[-1][batch_indices, rejected_last_idx, :]\n",
        "\n",
        "                chosen_rewards = model.reward_head(chosen_last_hidden).squeeze(-1)\n",
        "                rejected_rewards = model.reward_head(rejected_last_hidden).squeeze(-1)\n",
        "\n",
        "                loss = -torch.log(torch.sigmoid(chosen_rewards - rejected_rewards)).mean()\n",
        "\n",
        "                loss = loss / gradient_accumulation_steps\n",
        "\n",
        "            accelerator.backward(loss)\n",
        "            epoch_loss += loss.item() * gradient_accumulation_steps  # un-scaled per-batch loss\n",
        "\n",
        "            # gradient clipping on unwrapped model parameters (PEFT might wrap)\n",
        "            if accelerator.sync_gradients:\n",
        "                clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            # optimizer step handled by accelerator when accumulate finishes\n",
        "            if accelerator.sync_gradients:\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                # Logging\n",
        "                if global_step % logging_steps == 0:\n",
        "                    # if (epoch == 2) :\n",
        "                    #   adjusted_steps = step - (starting_step * gradient_accumulation_steps - (epoch * len(train_dataloader)))\n",
        "                    # else:\n",
        "                    #   adjusted_steps = step\n",
        "                    adjusted_steps = step\n",
        "                    avg_loss = epoch_loss * train_batch_size / ((adjusted_steps + 1) * train_batch_size) #epoch_loss is sum of per-batch mean losses\n",
        "                    #(step + 1) * train_batch_size is number of samples seen so far\n",
        "                    print(f\"Epoch {epoch+1} Step {global_step} loss {avg_loss:.4f}\")\n",
        "                    writer.add_scalar(\"train/loss\", avg_loss, global_step)\n",
        "                    writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[0], global_step)\n",
        "\n",
        "                # Evaluation\n",
        "                if global_step % eval_steps == 0:\n",
        "                    model.eval()\n",
        "                    total_eval_loss = 0.0\n",
        "                    batches = 0\n",
        "                    accuracy = 0.0\n",
        "                    for eval_batch in eval_dataloader:\n",
        "                        with torch.no_grad():\n",
        "                            with torch.cuda.amp.autocast(enabled=accelerator.mixed_precision==\"fp16\"):\n",
        "                                chosen_outputs = model(\n",
        "                                    input_ids=batch[\"chosen_input_ids\"],\n",
        "                                    attention_mask=batch[\"chosen_attention_mask\"],\n",
        "                                    output_hidden_states=True\n",
        "                                )\n",
        "                                rejected_outputs = model(\n",
        "                                    input_ids=batch[\"rejected_input_ids\"],\n",
        "                                    attention_mask=batch[\"rejected_attention_mask\"],\n",
        "                                    output_hidden_states=True\n",
        "                                )\n",
        "                                chosen_last_idx = batch[\"chosen_attention_mask\"].sum(dim=1) - 1\n",
        "                                rejected_last_idx = batch[\"rejected_attention_mask\"].sum(dim=1) - 1\n",
        "                                batch_indices = torch.arange(chosen_outputs.hidden_states[-1].size(0), device=chosen_outputs.hidden_states[-1].device)\n",
        "\n",
        "                                chosen_last_hidden = chosen_outputs.hidden_states[-1][batch_indices, chosen_last_idx, :]\n",
        "                                rejected_last_hidden = rejected_outputs.hidden_states[-1][batch_indices, rejected_last_idx, :]\n",
        "\n",
        "                                chosen_rewards = model.reward_head(chosen_last_hidden).squeeze(-1)\n",
        "                                rejected_rewards = model.reward_head(rejected_last_hidden).squeeze(-1)\n",
        "\n",
        "                                loss = -torch.log(torch.sigmoid(chosen_rewards - rejected_rewards)).mean()\n",
        "\n",
        "                                accuracy += (chosen_rewards > rejected_rewards).float().mean()\n",
        "\n",
        "                                total_eval_loss += loss.item()\n",
        "                                batches += 1\n",
        "                    avg_eval_loss = total_eval_loss / batches\n",
        "                    accuracy = accuracy / batches\n",
        "                    print(f\"*** Eval at step {global_step}: loss {avg_eval_loss:.4f}\")\n",
        "                    print(f\"*** Accuracy at step {global_step}: accuracy {accuracy:.4f}\")\n",
        "                    writer.add_scalar(\"eval/loss\", avg_eval_loss, global_step)\n",
        "                    model.train()\n",
        "\n",
        "                # Save checkpoint\n",
        "                if global_step % save_steps == 0:\n",
        "                    # Save PEFT adapter and accelerator state\n",
        "                    save_training_state(save_dir, global_step, accelerator, optimizer, scheduler, scaler)\n",
        "\n",
        "    # epoch end\n",
        "    # Save checkpoint at epoch end\n",
        "    save_training_state(save_dir, f\"epoch-{epoch+1}\", accelerator, optimizer, scheduler, scaler)\n",
        "\n",
        "# final save\n",
        "save_training_state(save_dir, f\"final-{global_step}\", accelerator, optimizer, scheduler, scaler)\n",
        "print(\"Training complete\")\n",
        "writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0DdZnixo2BY",
        "outputId": "0781cbb5-6b2d-4b01-aeec-30bca59c9c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-384877413.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=accelerator.mixed_precision==\"fp16\"):\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Step 50 loss 1.0223\n",
            "Epoch 1 Step 100 loss 1.0031\n",
            "Epoch 1 Step 150 loss 0.9963\n",
            "Epoch 1 Step 200 loss 0.9835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-384877413.py:73: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=accelerator.mixed_precision==\"fp16\"):\n",
            "WARNING:accelerate.utils.other:Removed shared tensor {'base_model.model.base_model.model.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Eval at step 200: loss 0.7077\n",
            "*** Accuracy at step 200: accuracy 0.5000\n",
            "Saved checkpoint to reward-model-accel/checkpoint-200\n",
            "Epoch 1 Step 250 loss 0.9704\n",
            "Epoch 1 Step 300 loss 0.9577\n",
            "Epoch 1 Step 350 loss 0.9470\n",
            "Epoch 1 Step 400 loss 0.9337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.utils.other:Removed shared tensor {'base_model.model.base_model.model.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Eval at step 400: loss 0.6510\n",
            "*** Accuracy at step 400: accuracy 0.5000\n",
            "Saved checkpoint to reward-model-accel/checkpoint-400\n",
            "Epoch 1 Step 450 loss 0.9236\n",
            "Epoch 1 Step 500 loss 0.9131\n",
            "Epoch 1 Step 550 loss 0.9027\n",
            "Epoch 1 Step 600 loss 0.8926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.utils.other:Removed shared tensor {'base_model.model.base_model.model.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Eval at step 600: loss 0.9065\n",
            "*** Accuracy at step 600: accuracy 0.5000\n",
            "Saved checkpoint to reward-model-accel/checkpoint-600\n",
            "Deleting old checkpoint: reward-model-accel/checkpoint-200\n",
            "Epoch 1 Step 650 loss 0.8825\n",
            "Epoch 1 Step 700 loss 0.8748\n",
            "Epoch 1 Step 750 loss 0.8656\n",
            "Epoch 1 Step 800 loss 0.8598\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.utils.other:Removed shared tensor {'base_model.model.base_model.model.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Eval at step 800: loss 0.7774\n",
            "*** Accuracy at step 800: accuracy 0.7500\n",
            "Saved checkpoint to reward-model-accel/checkpoint-800\n",
            "Deleting old checkpoint: reward-model-accel/checkpoint-400\n",
            "Epoch 1 Step 850 loss 0.8535\n",
            "Epoch 1 Step 900 loss 0.8473\n",
            "Epoch 1 Step 950 loss 0.8409\n",
            "Epoch 1 Step 1000 loss 0.8355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.utils.other:Removed shared tensor {'base_model.model.base_model.model.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Eval at step 1000: loss 0.8490\n",
            "*** Accuracy at step 1000: accuracy 0.2500\n",
            "Saved checkpoint to reward-model-accel/checkpoint-1000\n",
            "Deleting old checkpoint: reward-model-accel/checkpoint-600\n",
            "Epoch 1 Step 1050 loss 0.8305\n",
            "Epoch 1 Step 1100 loss 0.8255\n",
            "Epoch 1 Step 1150 loss 0.8215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.utils.other:Removed shared tensor {'base_model.model.base_model.model.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to reward-model-accel/checkpoint-epoch-1\n",
            "Deleting old checkpoint: reward-model-accel/checkpoint-800\n",
            "Epoch 2 Step 1200 loss 0.7021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.utils.other:Removed shared tensor {'base_model.model.base_model.model.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Eval at step 1200: loss 0.6293\n",
            "*** Accuracy at step 1200: accuracy 0.7500\n",
            "Saved checkpoint to reward-model-accel/checkpoint-1200\n",
            "Deleting old checkpoint: reward-model-accel/checkpoint-1000\n",
            "Epoch 2 Step 1250 loss 0.7261\n",
            "Epoch 2 Step 1300 loss 0.7290\n",
            "Epoch 2 Step 1350 loss 0.7250\n",
            "Epoch 2 Step 1400 loss 0.7239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.utils.other:Removed shared tensor {'base_model.model.base_model.model.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Eval at step 1400: loss 0.8421\n",
            "*** Accuracy at step 1400: accuracy 0.2500\n",
            "Saved checkpoint to reward-model-accel/checkpoint-1400\n",
            "Deleting old checkpoint: reward-model-accel/checkpoint-epoch-1\n",
            "Epoch 2 Step 1450 loss 0.7209\n",
            "Epoch 2 Step 1500 loss 0.7197\n",
            "Epoch 2 Step 1550 loss 0.7190\n",
            "Epoch 2 Step 1600 loss 0.7185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.utils.other:Removed shared tensor {'base_model.model.base_model.model.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Eval at step 1600: loss 0.7821\n",
            "*** Accuracy at step 1600: accuracy 0.7500\n",
            "Saved checkpoint to reward-model-accel/checkpoint-1600\n",
            "Deleting old checkpoint: reward-model-accel/checkpoint-1200\n",
            "Epoch 2 Step 1650 loss 0.7174\n",
            "Epoch 2 Step 1700 loss 0.7174\n",
            "Epoch 2 Step 1750 loss 0.7173\n",
            "Epoch 2 Step 1800 loss 0.7176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.utils.other:Removed shared tensor {'base_model.model.base_model.model.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Eval at step 1800: loss 0.5481\n",
            "*** Accuracy at step 1800: accuracy 0.5000\n",
            "Saved checkpoint to reward-model-accel/checkpoint-1800\n",
            "Deleting old checkpoint: reward-model-accel/checkpoint-1400\n",
            "Epoch 2 Step 1850 loss 0.7172\n",
            "Epoch 2 Step 1900 loss 0.7173\n",
            "Epoch 2 Step 1950 loss 0.7166\n",
            "Epoch 2 Step 2000 loss 0.7154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.utils.other:Removed shared tensor {'base_model.model.base_model.model.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Eval at step 2000: loss 0.6420\n",
            "*** Accuracy at step 2000: accuracy 0.7500\n",
            "Saved checkpoint to reward-model-accel/checkpoint-2000\n",
            "Deleting old checkpoint: reward-model-accel/checkpoint-1600\n",
            "Epoch 2 Step 2050 loss 0.7151\n",
            "Epoch 2 Step 2100 loss 0.7149\n",
            "Epoch 2 Step 2150 loss 0.7145\n",
            "Epoch 2 Step 2200 loss 0.7145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.utils.other:Removed shared tensor {'base_model.model.base_model.model.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Eval at step 2200: loss 0.6690\n",
            "*** Accuracy at step 2200: accuracy 0.5000\n",
            "Saved checkpoint to reward-model-accel/checkpoint-2200\n",
            "Deleting old checkpoint: reward-model-accel/checkpoint-1800\n",
            "Epoch 2 Step 2250 loss 0.7145\n",
            "Epoch 2 Step 2300 loss 0.7142\n",
            "Epoch 2 Step 2350 loss 0.7142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.utils.other:Removed shared tensor {'base_model.model.base_model.model.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to reward-model-accel/checkpoint-epoch-2\n",
            "Deleting old checkpoint: reward-model-accel/checkpoint-2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.utils.other:Removed shared tensor {'base_model.model.base_model.model.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to reward-model-accel/checkpoint-final-2384\n",
            "Deleting old checkpoint: reward-model-accel/checkpoint-2200\n",
            "Training complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "total_eval_loss = 0.0\n",
        "batches = 0\n",
        "accuracy = 0.0\n",
        "for eval_batch in eval_dataloader:\n",
        "    with torch.no_grad():\n",
        "        with torch.cuda.amp.autocast(enabled=accelerator.mixed_precision==\"fp16\"):\n",
        "            chosen_outputs = model(\n",
        "                input_ids=batch[\"chosen_input_ids\"],\n",
        "                attention_mask=batch[\"chosen_attention_mask\"],\n",
        "                output_hidden_states=True\n",
        "            )\n",
        "            rejected_outputs = model(\n",
        "                input_ids=batch[\"rejected_input_ids\"],\n",
        "                attention_mask=batch[\"rejected_attention_mask\"],\n",
        "                output_hidden_states=True\n",
        "            )\n",
        "            chosen_last_idx = batch[\"chosen_attention_mask\"].sum(dim=1) - 1\n",
        "            rejected_last_idx = batch[\"rejected_attention_mask\"].sum(dim=1) - 1\n",
        "            batch_indices = torch.arange(chosen_outputs.hidden_states[-1].size(0), device=chosen_outputs.hidden_states[-1].device)\n",
        "\n",
        "            chosen_last_hidden = chosen_outputs.hidden_states[-1][batch_indices, chosen_last_idx, :]\n",
        "            rejected_last_hidden = rejected_outputs.hidden_states[-1][batch_indices, rejected_last_idx, :]\n",
        "\n",
        "            chosen_rewards = model.reward_head(chosen_last_hidden).squeeze(-1)\n",
        "            rejected_rewards = model.reward_head(rejected_last_hidden).squeeze(-1)\n",
        "\n",
        "            loss = -torch.log(torch.sigmoid(chosen_rewards - rejected_rewards)).mean()\n",
        "\n",
        "            accuracy += (chosen_rewards > rejected_rewards).float().mean()\n",
        "\n",
        "            total_eval_loss += loss.item()\n",
        "            batches += 1\n",
        "avg_eval_loss = total_eval_loss / batches\n",
        "accuracy = accuracy / batches\n",
        "print(f\"*** Eval at step {global_step}: loss {avg_eval_loss:.4f}\")\n",
        "print(f\"*** Accuracy at step {global_step}: accuracy {accuracy:.4f}\")\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhW7QaIIlPiu",
        "outputId": "10804ff4-1f5b-43b1-aecb-5526809161f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2751101777.py:7: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=accelerator.mixed_precision==\"fp16\"):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Eval at step 2384: loss 0.6076\n",
            "*** Accuracy at step 2384: accuracy 0.7500\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): PeftModelForCausalLM(\n",
              "      (base_model): LoraModel(\n",
              "        (model): GPT2LMHeadModel(\n",
              "          (transformer): GPT2Model(\n",
              "            (wte): Embedding(50257, 768)\n",
              "            (wpe): Embedding(1024, 768)\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "            (h): ModuleList(\n",
              "              (0-11): 12 x GPT2Block(\n",
              "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (attn): GPT2Attention(\n",
              "                  (c_attn): lora.Linear(\n",
              "                    (base_layer): Conv1D(nf=2304, nx=768)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=8, out_features=2304, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (c_proj): Conv1D(nf=768, nx=768)\n",
              "                  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (mlp): GPT2MLP(\n",
              "                  (c_fc): Conv1D(nf=3072, nx=768)\n",
              "                  (c_proj): Conv1D(nf=768, nx=3072)\n",
              "                  (act): NewGELUActivation()\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (reward_head): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Push to the Hugging Face Hub"
      ],
      "metadata": {
        "id": "wvCX2CJGoUwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_dir = \"reward-model-accel/checkpoint-final-2384\"\n",
        "adapter_dir = f\"{ckpt_dir}/adapter\"\n",
        "\n",
        "model.load_adapter(adapter_dir, adapter_name=\"default\")\n",
        "\n",
        "reward_head_path = f\"{ckpt_dir}/reward_head.pt\"\n",
        "state_dict = torch.load(reward_head_path, map_location=\"cpu\")\n",
        "model.reward_head.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "repo_name = \"ArnavM3434/reward-model-gpt2\"\n",
        "#model.push_to_hub(repo_name)"
      ],
      "metadata": {
        "id": "_yT_sPCXnZTB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "repo_name = \"ArnavM3434/reward-model-gpt2\"\n",
        "reward_head_path = \"reward-model-accel/checkpoint-final-2384/reward_head.pt\"\n",
        "\n",
        "api = HfApi()\n",
        "api.upload_file(\n",
        "    path_or_fileobj=reward_head_path,\n",
        "    path_in_repo=\"reward_head.pt\",\n",
        "    repo_id=repo_name,\n",
        "    token=True\n",
        ")\n",
        "\n",
        "print(\"reward_head.pt uploaded to Hugging Face Hub\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131,
          "referenced_widgets": [
            "75ffd55a00d4423cbfbcc8a25961f407",
            "38c5fc94a5f845b4882ad4daa7b05447",
            "ec723532d01a442b85b9c9e5dfe1cfed",
            "bd1f9f4090e94b3a89e2b1579f06143e",
            "f713a89d1dae40e48c34b79bad1ea513",
            "9621bd073fbc4d47bf24714fcd205618",
            "09d5c067920345f0a1e734558b9e6fe1",
            "03a1f4deb7fe4a6fb932e22461d6493a",
            "7a3e12121a004bce9e22c06677530cd6",
            "d5cf51f9e34449a4aeea0bcfe95e5193",
            "f317a9f380654e4cb31dd1fbd4a106f6",
            "93fccc59c0a8402a994bd780725cc44d",
            "7b8c82de5eb747b5953ac0de3c53c3ff",
            "f07471f17b694bcd86e08d23d9ed2b63",
            "dfeba74e35954582b36b155c8675c81f",
            "958bc4374ae349daadf5c0284ab31f28",
            "3b08d19df0c543a39db42e1323eee78c",
            "97ab0edca7e6465e9a3e2f276a6dd993",
            "7390b2c5b24c4378b6ee48fd4cc48ae5",
            "8164fe9e9cc7497cbcb8c5c2b3290bed",
            "d845051901eb4bcbac2b23db861775d3",
            "81e3f0548b6e4c99b41c85ac8a5d33d8",
            "951efd275c6341e7a087c30cfb90146a",
            "cb7ae6e5c3a44e1ea0166079a34e92e6",
            "33f7791acc4549d49f7aa67466168738",
            "e45f24de3d3c48e494a49cf212d99ce0",
            "82fa1afcb5284822878951bab69eb5c4",
            "61556fa6939c4ae0a61e1505aaef4693",
            "64eb124e402548848ff772d06e22141e",
            "9710a2f7cce343309180f72830b4c83d",
            "e796681d5ac3461f8372acfba2513eaa",
            "c01ed1105caa41cc815f4c96b62138ac",
            "5d54913b6b064e9c9cf1e6690e91608d"
          ]
        },
        "id": "SjTqjonTTWMc",
        "outputId": "7f6b981b-5081-4099-e2c3-48470df7a9da"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75ffd55a00d4423cbfbcc8a25961f407"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93fccc59c0a8402a994bd780725cc44d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...final-2384/reward_head.pt: 100%|##########| 5.00kB / 5.00kB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "951efd275c6341e7a087c30cfb90146a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reward_head.pt uploaded to Hugging Face Hub\n"
          ]
        }
      ]
    }
  ]
}