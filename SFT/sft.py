# -*- coding: utf-8 -*-
"""SFT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LWTQbqL_0RVG99nGI3YbxUHSK9YD6w5U
"""

!pip install transformers datasets accelerate trl peft tensorboard

import os
import math
import json
from pathlib import Path
from typing import Dict, Any
import glob
import shutil

import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torch.nn.utils import clip_grad_norm_

from peft import LoraConfig, get_peft_model, PeftModel

from torch.utils.tensorboard import SummaryWriter

from transformers import get_linear_schedule_with_warmup

from google.colab import drive
drive.mount('/content/drive')

workspace = 'RLHF-Reproduction'

import os
os.chdir(f"/content/drive/My Drive/{workspace}")
print("Current working dir:", os.getcwd())

"""# Pretrained GPT2 Behavior"""

model_name = "gpt2"

from transformers import AutoTokenizer, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto")

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

print("EOS token id:", tokenizer.eos_token_id)
print("Pad token id:", tokenizer.pad_token_id)

print(tokenizer.bos_token, tokenizer.eos_token, tokenizer.pad_token)
print("Vocab size:", len(tokenizer))
print(type(tokenizer))

text = "testing tokenizer"
tokens = tokenizer.tokenize(text)
ids = tokenizer.encode(text)
print(tokens)
print(ids)

prompt = "Complete this sentence. The cat jumped over "
inputs = tokenizer(prompt, return_tensors="pt")

print("Input IDs:", inputs["input_ids"])
print("Shape:", inputs["input_ids"].shape)

import torch
with torch.no_grad():
    outputs = model(**inputs)


logits = outputs.logits        # [batch, seq_len, vocab_size]

print("Logits shape:", logits.shape)

generated_ids = model.generate(
    **inputs,
    max_new_tokens=60,
    do_sample=True,
    temperature=0.7,
    top_p=0.9
)

print(generated_ids.shape)

print(tokenizer.decode(generated_ids[0]))

generated_ids = model.generate(           #beam search
    **inputs,
    max_new_tokens=40,
    num_beams=5,             # beam width
    early_stopping=True,     # stop when EOS is reached
    no_repeat_ngram_size=2,  # optional: avoid repeated phrases
)
print(generated_ids.shape)

for seq in generated_ids:
    print(tokenizer.decode(seq, skip_special_tokens=True))

"""##Alpaca Dataset"""

from datasets import load_dataset
dataset = load_dataset("tatsu-lab/alpaca")

dataset

from datasets import DatasetDict
split = dataset['train'].train_test_split(test_size=0.05, seed=42)
dataset = DatasetDict({
    'train': split['train'],
    'validation': split['test']
})

dataset

train_ds = dataset['train']
train_ds[0]

train_ds[1]

def compute_lengths(example):
    tokens = tokenizer(example['text']).input_ids
    return {'length': len(tokens)}

lengths = dataset['train'].map(compute_lengths)
max_len = max(lengths['length'])
print("Max token length in train set:", max_len)

import numpy as np

arr = np.array(lengths["length"])
count = np.sum(arr <= 1024)
count

lengths = dataset['validation'].map(compute_lengths)
max_len = max(lengths['length'])
print("Max token length in validation set:", max_len)

print(model.config.n_positions)

"""Filter out the 2 examples that exceed 1024 tokens"""

train_ds = train_ds.filter(lambda x: len(tokenizer(x["text"]).input_ids) <= 1024)
print(train_ds.num_rows)  # should be 49399

val_ds = dataset['validation'].filter(lambda x: len(tokenizer(x["text"]).input_ids) <= 1024)
print(val_ds.num_rows)  # should be 2601

max_length = 1024
def tokenize_batch(batch): #for labels only want to compute loss for responses
    texts = batch["text"]

    input_ids_list = []
    attention_masks_list = []
    label_list = []

    for text in texts:
        response_start = text.find("### Response:")
        if response_start == -1:
            print("FAILURE")
            response_start = len(text)

        # Split instruction+input and response
        prompt = text[:response_start]
        response = text[response_start:]

        # Tokenize together to preserve continuity
        tokenized = tokenizer(
            prompt + response,
            truncation=True,
            max_length=max_length,
            padding="max_length",
        )

        # Compute how many tokens belong to the prompt (for masking)
        prompt_tokens = tokenizer(prompt, truncation=True, max_length=max_length)["input_ids"]
        prompt_length = len(prompt_tokens)

        # Copy input_ids as labels, but mask out prompt tokens
        labels = tokenized["input_ids"].copy()
        labels[:prompt_length] = [-100] * prompt_length  # ignore prompt in loss

        for i in range(len(labels)):  #only want the first eos token to contribute to the loss, none of the padding tokens
          if tokenized["attention_mask"][i] == 0:
            labels[i] = -100

        input_ids_list.append(tokenized["input_ids"])
        attention_masks_list.append(tokenized["attention_mask"])
        label_list.append(labels)

    return {
        "input_ids": input_ids_list,
        "attention_mask": attention_masks_list,
        "labels": label_list,
    }


tokenized_train_ds= train_ds.map(
    tokenize_batch,
    batched=True,
    batch_size=1000,
    remove_columns=train_ds.column_names
)

tokenized_val_ds = val_ds.map(
    tokenize_batch,
    batched=True,
    batch_size=1000,
    remove_columns=val_ds.column_names
)

print(tokenized_train_ds.column_names)

print(len(tokenized_train_ds[0]["input_ids"]))

"""## Fine Tuning on Alpaca Dataset"""

train_batch_size = 4
gradient_accumulation_steps = 4
num_epochs = 3
lr = 2e-4
weight_decay = 0.0
warmup_steps = 100
save_dir = "./gpt2-alpaca-sft-accel"
checkpoint_prefix = "checkpoint"
save_steps = 200
logging_steps = 50
eval_steps = 200
fp16 = True
num_workers = 2
max_checkpoints = 2

from accelerate import Accelerator
accelerator = Accelerator(
    mixed_precision="fp16",
    gradient_accumulation_steps=gradient_accumulation_steps  # <-- this is where 4 is set
)
device = accelerator.device
print("Running on", device)

def load_peft_adapter_if_exists(model, ckpt_dir: str, accelerator: Accelerator):
    adapter_path = Path(ckpt_dir) / "adapter"
    if adapter_path.exists():
        # Load the PEFT adapter weights into model
        # model must be a base model wrapped with get_peft_model earlier
        print(f"Loading PEFT adapter from {adapter_path}")
        model = PeftModel.from_pretrained(model, adapter_path, device_map={"": accelerator.device})
    return model

def ensure_dir(path):
    Path(path).mkdir(parents=True, exist_ok=True)

def save_training_state(save_dir: str, step: int, accelerator, optimizer, scheduler, scaler):
    save_dir = Path(save_dir)
    ckpt_dir = save_dir / f"{checkpoint_prefix}-{step}"
    ensure_dir(ckpt_dir)

    # Save PEFT adapter weights (preferred)
    model_to_save = accelerator.unwrap_model(model)
    peft_save_dir = ckpt_dir / "adapter"
    model_to_save.save_pretrained(peft_save_dir)

    # Save optimizer/scheduler/scaler states
    accelerator.save_state(str(ckpt_dir / "acc_state"))

    # Save meta info
    meta = {"step": step}
    (ckpt_dir / "meta.json").write_text(json.dumps(meta))

    print(f"Saved checkpoint to {ckpt_dir}")

    # --- Delete old checkpoints if more than MAX_CHECKPOINTS ---
    all_ckpts = sorted(glob.glob(str(save_dir / f"{checkpoint_prefix}-*")),
                       key=lambda x: Path(x).stat().st_mtime)
    while len(all_ckpts) > max_checkpoints:
        old_ckpt = Path(all_ckpts.pop(0))
        print(f"Deleting old checkpoint: {old_ckpt}")
        shutil.rmtree(old_ckpt)

def collate_fn(batch):
    # batch is a list of dicts with input_ids, attention_mask, labels (each already list length max_length)
    input_ids = torch.tensor([b["input_ids"] for b in batch], dtype=torch.long)
    attention_mask = torch.tensor([b["attention_mask"] for b in batch], dtype=torch.long)
    labels = torch.tensor([b["labels"] for b in batch], dtype=torch.long)
    return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels}

def inspect_trainable_params(model):
    total = 0
    trainable = 0
    details = []
    for n, p in model.named_parameters():
        total += p.numel()
        if p.requires_grad:
            trainable += p.numel()
            details.append(n)
    print(f"Trainable params: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)")
    print("Example trainable params:", details[:20])
    return details

ensure_dir(save_dir)

writer = SummaryWriter(log_dir=os.path.join(save_dir, "tensorboard"))

LORA_CONFIG = dict(
    r=8,
    lora_alpha=32,
    target_modules=["c_attn"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model.gradient_checkpointing_enable()

lora_config = LoraConfig(**LORA_CONFIG)

model = get_peft_model(model, lora_config)

print("Before:")
trainable_before = inspect_trainable_params(model)

resume_checkpoint_dir = "./gpt2-alpaca-sft-accel/checkpoint-6200"
# load peft adapter weights if present
model = load_peft_adapter_if_exists(model, resume_checkpoint_dir, accelerator)

# Make sure LoRA parameters are trainable
for name, param in model.named_parameters():
    if "lora_" in name:
        param.requires_grad = True
    else:
        param.requires_grad = False  # freeze base model

print("After:")
trainable_after = inspect_trainable_params(model)

for n, p in model.named_parameters():
    if p.requires_grad:
        p.data = p.data.to(accelerator.device)  # ensure params are on same device

train_dataloader = DataLoader(tokenized_train_ds, shuffle=True, collate_fn=collate_fn,batch_size=train_batch_size, num_workers=num_workers)
eval_dataloader = DataLoader(tokenized_val_ds, shuffle=False, collate_fn=collate_fn, batch_size=train_batch_size, num_workers=num_workers)

no_decay = ["bias", "LayerNorm.weight"]
param_groups = [
        {"params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], "weight_decay": weight_decay},
        {"params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], "weight_decay": 0.0},
    ]
optimizer = AdamW(param_groups, lr=lr)

#optimizer = AdamW([p for p in model.parameters() if p.requires_grad], lr=lr)

total_train_steps = math.ceil(len(train_dataloader) * num_epochs / gradient_accumulation_steps)

scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_train_steps,
    )

model, optimizer, train_dataloader, eval_dataloader, scheduler = accelerator.prepare(
        model, optimizer, train_dataloader, eval_dataloader, scheduler
    )

starting_step = 0
scaler = torch.cuda.amp.GradScaler(enabled=(accelerator.state.mixed_precision == "fp16"))

# load accelerator state (this will restore optimizer/scheduler/scaler if saved via accelerator.save_state)
acc_state_dir = Path(resume_checkpoint_dir) / "acc_state"
if acc_state_dir.exists():
    print("Loading accelerator state from", acc_state_dir)
    #accelerator.load_state(str(acc_state_dir))
    # try to read meta to get step
    meta_file = Path(resume_checkpoint_dir) / "meta.json"
    if meta_file.exists():
        meta = json.loads(meta_file.read_text())
        starting_step = int(meta.get("step", 0))
    print("Resumed step:", starting_step)
else:
    print("No accelerator state found in checkpoint; only adapter weights restored (if existed).")

print(starting_step)

global_step = starting_step
model.train()
print("***** Running training *****")
print(f"  Num examples = {len(tokenized_train_ds)}")
print(f"  Num Epochs = {num_epochs}")
print(f"  Instantaneous batch size per device = {train_batch_size}")
print(f"  Gradient Accumulation steps = {gradient_accumulation_steps}")
print(f"  Total optimization steps = {total_train_steps}")

for epoch in range(num_epochs):
    epoch_loss = 0.0
    for step, batch in enumerate(train_dataloader):

        #in case we are resuming training
        accumulated_steps = (epoch * len(train_dataloader) + step) // gradient_accumulation_steps
        if accumulated_steps < starting_step:
            continue

        # batch: dict of tensors
        with accelerator.accumulate(model):
            # forward with amp context
            with torch.cuda.amp.autocast(enabled=accelerator.mixed_precision=="fp16"):
                outputs = model(
                    input_ids=batch["input_ids"],
                    attention_mask=batch["attention_mask"],
                    labels=batch["labels"],
                )
                loss = outputs.loss
                # scale loss to account for gradient accumulation
                loss = loss / gradient_accumulation_steps

            accelerator.backward(loss)
            epoch_loss += loss.item() * gradient_accumulation_steps  # un-scaled per-batch loss

            # gradient clipping on unwrapped model parameters (PEFT might wrap)
            if accelerator.sync_gradients:
                clip_grad_norm_(model.parameters(), max_norm=1.0)

            # optimizer step handled by accelerator when accumulate finishes
            if accelerator.sync_gradients:
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
                global_step += 1

                # Logging
                if global_step % logging_steps == 0:
                    if (epoch == 2) :
                      adjusted_steps = step - (starting_step * gradient_accumulation_steps - (epoch * len(train_dataloader)))
                    else:
                      adjusted_steps = step
                    avg_loss = epoch_loss * train_batch_size / ((adjusted_steps + 1) * train_batch_size) #epoch_loss is sum of per-batch mean losses
                    #(step + 1) * train_batch_size is number of samples seen so far
                    print(f"Epoch {epoch+1} Step {global_step} loss {avg_loss:.4f}")
                    writer.add_scalar("train/loss", avg_loss, global_step)
                    writer.add_scalar("train/lr", scheduler.get_last_lr()[0], global_step)

                # Evaluation
                if global_step % eval_steps == 0:
                    model.eval()
                    total_eval_loss = 0.0
                    batches = 0
                    for eval_batch in eval_dataloader:
                        with torch.no_grad():
                            with torch.cuda.amp.autocast(enabled=accelerator.mixed_precision=="fp16"):
                                out = model(
                                    input_ids=eval_batch["input_ids"],
                                    attention_mask=eval_batch["attention_mask"],
                                    labels=eval_batch["labels"],
                                )
                                total_eval_loss += out.loss.item()
                                batches += 1
                    avg_eval_loss = total_eval_loss / batches
                    ppl = math.exp(avg_eval_loss) if avg_eval_loss < 20 else float("inf")
                    print(f"*** Eval at step {global_step}: loss {avg_eval_loss:.4f}, ppl {ppl:.2f}")
                    writer.add_scalar("eval/loss", avg_eval_loss, global_step)
                    writer.add_scalar("eval/ppl", ppl, global_step)
                    model.train()

                # Save checkpoint
                if global_step % save_steps == 0:
                    # Save PEFT adapter and accelerator state
                    save_training_state(save_dir, global_step, accelerator, optimizer, scheduler, scaler)

    # epoch end
    # Save checkpoint at epoch end
    save_training_state(save_dir, f"epoch-{epoch+1}", accelerator, optimizer, scheduler, scaler)

# final save
save_training_state(save_dir, f"final-{global_step}", accelerator, optimizer, scheduler, scaler)
print("Training complete")
writer.close()