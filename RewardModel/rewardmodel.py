# -*- coding: utf-8 -*-
"""RewardModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gp7LuJ83MzE6TiiHBhVhuL7mga95iFtB

##Load the SFT Model and Add Scalar Reward Head
"""

!pip install transformers datasets accelerate trl peft tensorboard

import os
import math
import json
from pathlib import Path
from typing import Dict, Any
import glob
import shutil

import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
import torch.nn as nn
from torch.nn.utils import clip_grad_norm_

from peft import LoraConfig, get_peft_model, PeftModel

from torch.utils.tensorboard import SummaryWriter

from transformers import get_linear_schedule_with_warmup

from google.colab import drive
drive.mount('/content/drive')

workspace = 'RewardModelTry2'

import os
os.chdir(f"/content/drive/My Drive/{workspace}")
print("Current working dir:", os.getcwd())

from huggingface_hub import login
login()

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, PeftModel

base_model = "gpt2"
adapter_path = "ArnavM3434/gpt2-alpaca-second-try"
model = AutoModelForCausalLM.from_pretrained(adapter_path)

tokenizer = AutoTokenizer.from_pretrained(base_model)
tokenizer.pad_token = tokenizer.eos_token

def inspect_trainable_params(model):
    total = 0
    trainable = 0
    details = []
    for n, p in model.named_parameters():
        total += p.numel()
        if p.requires_grad:
            trainable += p.numel()
            details.append(n)
    print(f"Trainable params: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)")
    print("Example trainable params:", details[:20])
    return details

trainable_before = inspect_trainable_params(model)

# Make sure LoRA parameters are trainable
for name, param in model.named_parameters():
    if "lora_" in name:
        param.requires_grad = True
    else:
        param.requires_grad = False  # freeze base model

trainable_after = inspect_trainable_params(model)

"""Add the Reward Model Head"""

model.reward_head = nn.Linear(model.config.hidden_size, 1)

for name, param in model.named_parameters():
    if "lora_" in name or "reward_head" in name:
        param.requires_grad = True
    else:
        param.requires_grad = False

inspect_trainable_params(model)

"""##Preference Dataset

Use Dahoas/rm-static
"""

from datasets import load_dataset
dataset = load_dataset("Dahoas/rm-static")

dataset

from datasets import DatasetDict
dataset = DatasetDict({
    'train': dataset['train'],
    'validation': dataset['test']
})

dataset

train_ds = dataset['train']
train_ds[0]

train_ds[1]

val_ds = dataset['validation']
val_ds[0]

import re

def keep_last_turn(example): #Want just single turn completions, since SFT model was trained on that distribution
    prompt = example["prompt"]

    human_turns = [m.start() for m in re.finditer(r'\bHuman:', prompt)]
    if not human_turns:
        return example

    last_human_start = human_turns[-1]
    trimmed_prompt = prompt[last_human_start:].strip()

    trimmed_prompt = re.sub(r'\s*\n+\s*Assistant:', ' Assistant:', trimmed_prompt)

    if not trimmed_prompt.endswith("Assistant:"):
        trimmed_prompt += " Assistant:"

    trimmed_prompt = re.sub(r'\s+', ' ', trimmed_prompt).strip()

    example["prompt"] = trimmed_prompt
    return example


train_ds = train_ds.map(
    keep_last_turn
)

val_ds = val_ds.map(
    keep_last_turn
)

train_ds[0]

val_ds[0]

max_length = 1024

def tokenize_example(example):
    chosen_text = example["prompt"] + example["chosen"]
    rejected_text = example["prompt"] + example["rejected"]

    chosen = tokenizer(
        chosen_text,
        truncation=True,
        max_length=max_length,
        padding="max_length",
    )
    rejected = tokenizer(
        rejected_text,
        truncation=True,
        max_length=max_length,
        padding="max_length",
    )

    return {
        "chosen_input_ids": chosen["input_ids"],
        "chosen_attention_mask": chosen["attention_mask"],
        "rejected_input_ids": rejected["input_ids"],
        "rejected_attention_mask": rejected["attention_mask"],
    }

tokenized_train_ds = train_ds.map(
    tokenize_example,
    remove_columns=train_ds.column_names,
)

tokenized_val_ds = val_ds.map(
    tokenize_example,
    remove_columns=val_ds.column_names,
)

print(tokenized_train_ds.column_names)

"""##Training the Reward Model"""

train_batch_size = 4
gradient_accumulation_steps = 4
num_epochs = 3
lr = 9e-6 #from InstructGPT paper
weight_decay = 0.0
warmup_steps = 100
save_dir = "./reward-model-accel"
checkpoint_prefix = "checkpoint"
save_steps = 200
logging_steps = 50
eval_steps = 200
fp16 = True
num_workers = 2
max_checkpoints = 2

from accelerate import Accelerator
accelerator = Accelerator(
    mixed_precision="fp16",
    gradient_accumulation_steps=gradient_accumulation_steps
)
device = accelerator.device
print("Running on", device)

def ensure_dir(path):
    Path(path).mkdir(parents=True, exist_ok=True)

def save_training_state(save_dir: str, step: int, accelerator, optimizer, scheduler, scaler):
    save_dir = Path(save_dir)
    ckpt_dir = save_dir / f"{checkpoint_prefix}-{step}"
    ensure_dir(ckpt_dir)

    # Save PEFT adapter weights (preferred)
    model_to_save = accelerator.unwrap_model(model)
    peft_save_dir = ckpt_dir / "adapter"
    model_to_save.save_pretrained(peft_save_dir)

    if hasattr(model, "reward_head"):
        reward_head_path = ckpt_dir / "reward_head.pt"
        torch.save(model.reward_head.state_dict(), reward_head_path)

    # Save optimizer/scheduler/scaler states
    accelerator.save_state(str(ckpt_dir / "acc_state"))

    # Save meta info
    meta = {"step": step}
    (ckpt_dir / "meta.json").write_text(json.dumps(meta))

    print(f"Saved checkpoint to {ckpt_dir}")

    # --- Delete old checkpoints if more than MAX_CHECKPOINTS ---
    all_ckpts = sorted(glob.glob(str(save_dir / f"{checkpoint_prefix}-*")),
                       key=lambda x: Path(x).stat().st_mtime)
    while len(all_ckpts) > max_checkpoints:
        old_ckpt = Path(all_ckpts.pop(0))
        print(f"Deleting old checkpoint: {old_ckpt}")
        shutil.rmtree(old_ckpt)

def collate_fn(batch):
    chosen_input_ids = torch.tensor([b["chosen_input_ids"] for b in batch], dtype=torch.long)
    chosen_attention_mask = torch.tensor([b["chosen_attention_mask"] for b in batch], dtype=torch.long)
    rejected_input_ids = torch.tensor([b["rejected_input_ids"] for b in batch], dtype=torch.long)
    rejected_attention_mask = torch.tensor([b["rejected_attention_mask"] for b in batch], dtype=torch.long)
    return {"chosen_input_ids": chosen_input_ids, "chosen_attention_mask": chosen_attention_mask, "rejected_input_ids": rejected_input_ids, "rejected_attention_mask": rejected_attention_mask}

ensure_dir(save_dir)

writer = SummaryWriter(log_dir=os.path.join(save_dir, "tensorboard"))

model.gradient_checkpointing_enable()

for n, p in model.named_parameters():
    if p.requires_grad:
        p.data = p.data.to(accelerator.device)  # ensure params are on same device

train_dataloader = DataLoader(tokenized_train_ds, shuffle=True, collate_fn=collate_fn,batch_size=train_batch_size, num_workers=num_workers)
eval_dataloader = DataLoader(tokenized_val_ds, shuffle=False, collate_fn=collate_fn, batch_size=train_batch_size, num_workers=num_workers)

no_decay = ["bias", "LayerNorm.weight"]
param_groups = [
        {"params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], "weight_decay": weight_decay},
        {"params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], "weight_decay": 0.0},
    ]
optimizer = AdamW(param_groups, lr=lr)

total_train_steps = math.ceil(len(train_dataloader) * num_epochs / gradient_accumulation_steps)

from transformers import get_cosine_schedule_with_warmup

scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=warmup_steps,
    num_training_steps=total_train_steps,
    num_cycles=0.5,
)

import torch
from pathlib import Path

def load_full_checkpoint(accelerator, model):
    ckpt_dir = Path("reward-model-accel/checkpoint-1400")
    adapter_dir = ckpt_dir / "adapter"
    reward_head_path = ckpt_dir / "reward_head.pt"
    acc_state_dir = ckpt_dir / "acc_state"

    model = PeftModel.from_pretrained(model, str(adapter_dir))

    if reward_head_path.exists():
        model.reward_head.load_state_dict(torch.load(reward_head_path, map_location="cpu"))
        print("Loaded reward_head state dict")

    accelerator.load_state(str(acc_state_dir))
    print(f"Loaded accelerator state from {acc_state_dir}")

    return model

load_full_checkpoint(accelerator, model)

model, optimizer, train_dataloader, eval_dataloader, scheduler = accelerator.prepare(
        model, optimizer, train_dataloader, eval_dataloader, scheduler
    )

inspect_trainable_params(model)

for name, param in model.named_parameters():
    if "lora_" in name or "reward_head" in name:
        param.requires_grad = True
    else:
        param.requires_grad = False

inspect_trainable_params(model)

starting_step = 1400
scaler = torch.cuda.amp.GradScaler(enabled=(accelerator.state.mixed_precision == "fp16"))

print(starting_step)

global_step = starting_step
model.train()
print("***** Running training *****")
print(f"  Num examples = {len(tokenized_train_ds)}")
print(f"  Num Epochs = {num_epochs}")
print(f"  Instantaneous batch size per device = {train_batch_size}")
print(f"  Gradient Accumulation steps = {gradient_accumulation_steps}")
print(f"  Total optimization steps = {total_train_steps}")

for epoch in range(num_epochs):
    epoch_loss = 0.0
    for step, batch in enumerate(train_dataloader):

        #in case we are resuming training
        accumulated_steps = (epoch * len(train_dataloader) + step) // gradient_accumulation_steps
        if accumulated_steps < starting_step:
            continue

        # batch: dict of tensors
        with accelerator.accumulate(model):
            # forward with amp context
            with torch.cuda.amp.autocast(enabled=accelerator.mixed_precision=="fp16"):
                chosen_outputs = model(
                    input_ids=batch["chosen_input_ids"],
                    attention_mask=batch["chosen_attention_mask"],
                    output_hidden_states=True
                )
                rejected_outputs = model(
                    input_ids=batch["rejected_input_ids"],
                    attention_mask=batch["rejected_attention_mask"],
                    output_hidden_states=True
                )
                chosen_last_idx = batch["chosen_attention_mask"].sum(dim=1) - 1
                rejected_last_idx = batch["rejected_attention_mask"].sum(dim=1) - 1
                batch_indices = torch.arange(chosen_outputs.hidden_states[-1].size(0), device=chosen_outputs.hidden_states[-1].device)

                chosen_last_hidden = chosen_outputs.hidden_states[-1][batch_indices, chosen_last_idx, :]
                rejected_last_hidden = rejected_outputs.hidden_states[-1][batch_indices, rejected_last_idx, :]

                chosen_rewards = model.reward_head(chosen_last_hidden).squeeze(-1)
                rejected_rewards = model.reward_head(rejected_last_hidden).squeeze(-1)

                chosen_rewards = chosen_rewards.clamp(-10, 10)
                rejected_rewards = rejected_rewards.clamp(-10, 10)

                loss = torch.nn.functional.softplus(-(chosen_rewards - rejected_rewards)).mean()

                loss = loss / gradient_accumulation_steps

            accelerator.backward(loss)
            epoch_loss += loss.item() * gradient_accumulation_steps  # un-scaled per-batch loss

            # gradient clipping on unwrapped model parameters (PEFT might wrap)
            if accelerator.sync_gradients:
                clip_grad_norm_(model.parameters(), max_norm=1.0)

            # optimizer step handled by accelerator when accumulate finishes
            if accelerator.sync_gradients:
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
                global_step += 1

                # Logging
                if global_step % logging_steps == 0:
                    adjusted_steps = step - (starting_step * gradient_accumulation_steps - (epoch * len(train_dataloader))) #how many batches you processed this epoch
                    avg_loss = epoch_loss * train_batch_size / ((adjusted_steps + 1) * train_batch_size) #epoch_loss is sum of per-batch mean losses
                    #(step + 1) * train_batch_size is number of samples seen so far
                    print(f"Epoch {epoch+1} Step {global_step} loss {avg_loss:.4f}")
                    writer.add_scalar("train/loss", avg_loss, global_step)
                    writer.add_scalar("train/lr", scheduler.get_last_lr()[0], global_step)

                # Evaluation
                if global_step % eval_steps == 0:
                    model.eval()
                    total_eval_loss = 0.0
                    batches = 0
                    accuracy = 0.0
                    for eval_batch in eval_dataloader:
                        with torch.no_grad():
                            with torch.cuda.amp.autocast(enabled=accelerator.mixed_precision=="fp16"):
                                chosen_outputs = model(
                                    input_ids=eval_batch["chosen_input_ids"],
                                    attention_mask=eval_batch["chosen_attention_mask"],
                                    output_hidden_states=True
                                )
                                rejected_outputs = model(
                                    input_ids=eval_batch["rejected_input_ids"],
                                    attention_mask=eval_batch["rejected_attention_mask"],
                                    output_hidden_states=True
                                )
                                chosen_last_idx = eval_batch["chosen_attention_mask"].sum(dim=1) - 1
                                rejected_last_idx = eval_batch["rejected_attention_mask"].sum(dim=1) - 1
                                batch_indices = torch.arange(chosen_outputs.hidden_states[-1].size(0), device=chosen_outputs.hidden_states[-1].device)

                                chosen_last_hidden = chosen_outputs.hidden_states[-1][batch_indices, chosen_last_idx, :]
                                rejected_last_hidden = rejected_outputs.hidden_states[-1][batch_indices, rejected_last_idx, :]

                                chosen_rewards = model.reward_head(chosen_last_hidden).squeeze(-1)
                                rejected_rewards = model.reward_head(rejected_last_hidden).squeeze(-1)

                                chosen_rewards = chosen_rewards.clamp(-10, 10)
                                rejected_rewards = rejected_rewards.clamp(-10, 10)

                                loss = torch.nn.functional.softplus(-(chosen_rewards - rejected_rewards)).mean()

                                accuracy += (chosen_rewards > rejected_rewards).float().mean()

                                total_eval_loss += loss.item()
                                batches += 1
                    avg_eval_loss = total_eval_loss / batches
                    accuracy = accuracy / batches
                    print(f"*** Eval at step {global_step}: loss {avg_eval_loss:.4f}")
                    print(f"*** Accuracy at step {global_step}: accuracy {accuracy:.4f}")
                    writer.add_scalar("eval/loss", avg_eval_loss, global_step)
                    model.train()

                # Save checkpoint
                if global_step % save_steps == 0:
                    # Save PEFT adapter and accelerator state
                    save_training_state(save_dir, global_step, accelerator, optimizer, scheduler, scaler)

    # epoch end
    # Save checkpoint at epoch end
    save_training_state(save_dir, f"epoch-{epoch+1}", accelerator, optimizer, scheduler, scaler)

# final save
save_training_state(save_dir, f"final-{global_step}", accelerator, optimizer, scheduler, scaler)
print("Training complete")
writer.close()

"""##Push to the Hugging Face Hub"""

ckpt_dir = Path("reward-model-accel/checkpoint-4000")
adapter_dir = ckpt_dir / "adapter"
reward_head_path = ckpt_dir / "reward_head.pt"

model = PeftModel.from_pretrained(model, str(adapter_dir))

if reward_head_path.exists():
    model.reward_head.load_state_dict(torch.load(reward_head_path, map_location="cpu"))
    print("Loaded reward_head state dict")

model.push_to_hub("ArnavM3434/reward-model-2nd-try")

from huggingface_hub import HfApi

repo_name = "ArnavM3434/reward-model-2nd-try"
reward_head_path = "reward-model-accel/checkpoint-4000/reward_head.pt"

api = HfApi()
api.upload_file(
    path_or_fileobj=reward_head_path,
    path_in_repo="reward_head.pt",
    repo_id=repo_name,
    token=True
)

print("reward_head.pt uploaded to Hugging Face Hub")